{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction to NLP Fundematals in TensorFlow\nNLP has the goal of deriving information out of natural language(could be sequences text or speech)\nAnother common term for NLP problems is sequence to squence problems(seq2seq)","metadata":{}},{"cell_type":"code","source":"!nvidia-smi -L","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:14.573565Z","iopub.execute_input":"2025-10-30T12:28:14.574128Z","iopub.status.idle":"2025-10-30T12:28:14.754761Z","shell.execute_reply.started":"2025-10-30T12:28:14.574103Z","shell.execute_reply":"2025-10-30T12:28:14.753906Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## Get helper functions","metadata":{}},{"cell_type":"code","source":"!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/refs/heads/main/extras/helper_functions.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:14.756412Z","iopub.execute_input":"2025-10-30T12:28:14.756680Z","iopub.status.idle":"2025-10-30T12:28:15.004769Z","shell.execute_reply.started":"2025-10-30T12:28:14.756657Z","shell.execute_reply":"2025-10-30T12:28:15.003967Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from helper_functions import create_tensorboard_callback,unzip_data,plot_loss_curves,compare_historys","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:15.006039Z","iopub.execute_input":"2025-10-30T12:28:15.006339Z","iopub.status.idle":"2025-10-30T12:28:15.011192Z","shell.execute_reply.started":"2025-10-30T12:28:15.006304Z","shell.execute_reply":"2025-10-30T12:28:15.010363Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Get a text dataset\nthe dataset we're going to be using is Kaggle's introduction to NLP dataset(text samples of tweets labelled as diaster or not diaster).","metadata":{}},{"cell_type":"code","source":"!wget https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:15.012891Z","iopub.execute_input":"2025-10-30T12:28:15.013302Z","iopub.status.idle":"2025-10-30T12:28:15.277561Z","shell.execute_reply.started":"2025-10-30T12:28:15.013283Z","shell.execute_reply":"2025-10-30T12:28:15.276667Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# unzip the data\nunzip_data(\"nlp_getting_started.zip\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:15.278790Z","iopub.execute_input":"2025-10-30T12:28:15.279033Z","iopub.status.idle":"2025-10-30T12:28:15.299409Z","shell.execute_reply.started":"2025-10-30T12:28:15.279011Z","shell.execute_reply":"2025-10-30T12:28:15.298748Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport pandas as pd\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:15.300424Z","iopub.execute_input":"2025-10-30T12:28:15.300760Z","iopub.status.idle":"2025-10-30T12:28:15.304853Z","shell.execute_reply.started":"2025-10-30T12:28:15.300736Z","shell.execute_reply":"2025-10-30T12:28:15.304024Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = pd.read_csv(\"train.csv\")\ntest_df = pd.read_csv(\"test.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:15.305708Z","iopub.execute_input":"2025-10-30T12:28:15.305955Z","iopub.status.idle":"2025-10-30T12:28:15.354131Z","shell.execute_reply.started":"2025-10-30T12:28:15.305938Z","shell.execute_reply":"2025-10-30T12:28:15.353552Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:15.354909Z","iopub.execute_input":"2025-10-30T12:28:15.355106Z","iopub.status.idle":"2025-10-30T12:28:15.365900Z","shell.execute_reply.started":"2025-10-30T12:28:15.355091Z","shell.execute_reply":"2025-10-30T12:28:15.365000Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# shuffle training dataframe\ntrain_df_shuffled = train_df.sample(frac=1,random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:15.366794Z","iopub.execute_input":"2025-10-30T12:28:15.367067Z","iopub.status.idle":"2025-10-30T12:28:15.379533Z","shell.execute_reply.started":"2025-10-30T12:28:15.367040Z","shell.execute_reply":"2025-10-30T12:28:15.378845Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df_shuffled.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:15.382730Z","iopub.execute_input":"2025-10-30T12:28:15.383070Z","iopub.status.idle":"2025-10-30T12:28:15.396331Z","shell.execute_reply.started":"2025-10-30T12:28:15.383040Z","shell.execute_reply":"2025-10-30T12:28:15.395692Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# what does the tesst dataframe look like\ntest_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:15.397107Z","iopub.execute_input":"2025-10-30T12:28:15.397370Z","iopub.status.idle":"2025-10-30T12:28:15.416302Z","shell.execute_reply.started":"2025-10-30T12:28:15.397342Z","shell.execute_reply":"2025-10-30T12:28:15.415441Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# how many examples of each class?\ntrain_df.target.value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:15.417323Z","iopub.execute_input":"2025-10-30T12:28:15.417721Z","iopub.status.idle":"2025-10-30T12:28:15.433159Z","shell.execute_reply.started":"2025-10-30T12:28:15.417692Z","shell.execute_reply":"2025-10-30T12:28:15.432332Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(train_df),len(test_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:15.434043Z","iopub.execute_input":"2025-10-30T12:28:15.434328Z","iopub.status.idle":"2025-10-30T12:28:15.446905Z","shell.execute_reply.started":"2025-10-30T12:28:15.434303Z","shell.execute_reply":"2025-10-30T12:28:15.446073Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Let's visualize some random training examples\nimport random\nrandom_index = random.randint(0,len(train_df)-5)\nfor row in train_df_shuffled[[\"text\",\"target\"]][random_index:random_index+5].itertuples():\n    _,text,target = row \n    print(f\"target: {target}\",\"(real diaster)\" if target > 0 else \"(not real diaster)\" )\n    print(f\"Text:\\n{text}\\n\")\n    print(10*\"__\",\"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:15.447882Z","iopub.execute_input":"2025-10-30T12:28:15.448108Z","iopub.status.idle":"2025-10-30T12:28:15.461236Z","shell.execute_reply.started":"2025-10-30T12:28:15.448092Z","shell.execute_reply":"2025-10-30T12:28:15.460492Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Split data into training and validation datasets","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:15.462172Z","iopub.execute_input":"2025-10-30T12:28:15.462584Z","iopub.status.idle":"2025-10-30T12:28:15.475532Z","shell.execute_reply.started":"2025-10-30T12:28:15.462559Z","shell.execute_reply":"2025-10-30T12:28:15.474775Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled[\"text\"].to_numpy(),\n                                                                            train_df_shuffled[\"target\"].to_numpy(),\n                                                                           test_size=0.1,\n                                                                           random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:15.476400Z","iopub.execute_input":"2025-10-30T12:28:15.476692Z","iopub.status.idle":"2025-10-30T12:28:15.489639Z","shell.execute_reply.started":"2025-10-30T12:28:15.476671Z","shell.execute_reply":"2025-10-30T12:28:15.488986Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(train_sentences),len(train_labels),len(val_sentences),len(val_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:15.490365Z","iopub.execute_input":"2025-10-30T12:28:15.490726Z","iopub.status.idle":"2025-10-30T12:28:15.504080Z","shell.execute_reply.started":"2025-10-30T12:28:15.490700Z","shell.execute_reply":"2025-10-30T12:28:15.503225Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# check the first ten samples\ntrain_sentences[:10],train_labels[:10]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:15.504988Z","iopub.execute_input":"2025-10-30T12:28:15.505243Z","iopub.status.idle":"2025-10-30T12:28:15.518246Z","shell.execute_reply.started":"2025-10-30T12:28:15.505218Z","shell.execute_reply":"2025-10-30T12:28:15.517481Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Converting text into number, machine learning don't know text.\nwhen dealing with a text problem, one of the first things you'll have to do before you can build a model is to convert your text to numbers.\nThere are a few ways to do this, namely:\n* *Tokenization* - direct mapping of token (a token could be a word, a character or in between) to number.\n* *Embedding* - Create a matrix of feature vector for each token ( the size of the feature vector can be defined and this embedding can be learned)","metadata":{}},{"cell_type":"markdown","source":"### Text vecorization(tokenization)","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import TextVectorization","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:15.519036Z","iopub.execute_input":"2025-10-30T12:28:15.519319Z","iopub.status.idle":"2025-10-30T12:28:15.531218Z","shell.execute_reply.started":"2025-10-30T12:28:15.519301Z","shell.execute_reply":"2025-10-30T12:28:15.530428Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text_vectorizer = TextVectorization(max_tokens=None, # how many word in the vocablary (automatically add <OOV>\n                                    standardize=\"lower_and_strip_punctuation\",\n                                    split=\"whitespace\",\n                                    ngrams=None, # create groups of n-words\n                                    output_mode=\"int\", # how to map token to number\n                                    output_sequence_length=None ,# how long do you want your sequence to be\n                                    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:15.531927Z","iopub.execute_input":"2025-10-30T12:28:15.532131Z","iopub.status.idle":"2025-10-30T12:28:15.553223Z","shell.execute_reply.started":"2025-10-30T12:28:15.532115Z","shell.execute_reply":"2025-10-30T12:28:15.552453Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# find the average number of tokens(words) in the training tweets\nround(sum([len(i.split()) for i in train_sentences]))/len(train_sentences)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:15.554121Z","iopub.execute_input":"2025-10-30T12:28:15.554398Z","iopub.status.idle":"2025-10-30T12:28:15.568685Z","shell.execute_reply.started":"2025-10-30T12:28:15.554376Z","shell.execute_reply":"2025-10-30T12:28:15.567997Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# setup  text vectorization variables\nmax_vocab_length =10000 # max number of words to have in our vocablary\nmax_length = 15 # max lenght our sequences will be ( e.g how many words from a tweet does a model see?)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:15.569468Z","iopub.execute_input":"2025-10-30T12:28:15.569706Z","iopub.status.idle":"2025-10-30T12:28:15.581544Z","shell.execute_reply.started":"2025-10-30T12:28:15.569690Z","shell.execute_reply":"2025-10-30T12:28:15.580723Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n                                    output_mode=\"int\",\n                                    output_sequence_length=max_length\n                                   )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:15.582282Z","iopub.execute_input":"2025-10-30T12:28:15.582551Z","iopub.status.idle":"2025-10-30T12:28:15.603647Z","shell.execute_reply.started":"2025-10-30T12:28:15.582524Z","shell.execute_reply":"2025-10-30T12:28:15.602833Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fit the text vectorizer to the training sentence\ntext_vectorizer.adapt(train_sentences)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:15.604495Z","iopub.execute_input":"2025-10-30T12:28:15.604769Z","iopub.status.idle":"2025-10-30T12:28:15.693554Z","shell.execute_reply.started":"2025-10-30T12:28:15.604748Z","shell.execute_reply":"2025-10-30T12:28:15.692730Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a sample sentence and tokenize it\nsample_sentence = \"There's a flood in my street!\"\ntext_vectorizer([sample_sentence])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:15.694663Z","iopub.execute_input":"2025-10-30T12:28:15.694859Z","iopub.status.idle":"2025-10-30T12:28:15.713782Z","shell.execute_reply.started":"2025-10-30T12:28:15.694844Z","shell.execute_reply":"2025-10-30T12:28:15.712919Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Choose a random sentece from the training dataset and tokenize it\nrandom_sentence = random.choice(train_sentences)\nprint(f\"Original text:\\n {random_sentence} \\n\\nVectorized Version: {text_vectorizer(random_sentence)} \")\ntext_vectorizer(random_sentence)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:15.714552Z","iopub.execute_input":"2025-10-30T12:28:15.714805Z","iopub.status.idle":"2025-10-30T12:28:15.745891Z","shell.execute_reply.started":"2025-10-30T12:28:15.714783Z","shell.execute_reply":"2025-10-30T12:28:15.745032Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# check if token is a sentence have the same int value across different sentences\nsample_sentence_two = \"schools are the best Western in Lit lit litterally.. LiTTErALLy..\"\ntext_vectorizer(sample_sentence_two)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:15.746726Z","iopub.execute_input":"2025-10-30T12:28:15.746931Z","iopub.status.idle":"2025-10-30T12:28:15.765651Z","shell.execute_reply.started":"2025-10-30T12:28:15.746914Z","shell.execute_reply":"2025-10-30T12:28:15.764940Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get the unique words in the vocabulary\nwords_in_vocab = text_vectorizer.get_vocabulary() # get all of the unique words in vocabulary\ntop_5_words = words_in_vocab[:5] # get the most common word\nbottom_5_words = words_in_vocab[-5:] # get the least common word\ntop_5_words,bottom_5_words,len(words_in_vocab)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:15.771301Z","iopub.execute_input":"2025-10-30T12:28:15.771636Z","iopub.status.idle":"2025-10-30T12:28:15.804971Z","shell.execute_reply.started":"2025-10-30T12:28:15.771613Z","shell.execute_reply":"2025-10-30T12:28:15.804351Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Creating and Embedding using an Embedding Layer\nTo make our embedding, we going to use tensorflow embedding layer\nThe parameters we care most about for our embedding layer:\n* `input_dim` = the size of our vocabulary\n* `output_dim` = the size of the output embedding vector,for example, a value of 100 would mean each token gets represented by a vector 100 long\n*  `input_length` = length of the sequences being passed to the embedding layer","metadata":{}},{"cell_type":"code","source":" from tensorflow.keras import layers\nembedding = layers.Embedding(input_dim=max_vocab_length,\n                            output_dim=128,\n                             input_length=max_length\n                            )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:15.805719Z","iopub.execute_input":"2025-10-30T12:28:15.805976Z","iopub.status.idle":"2025-10-30T12:28:15.813195Z","shell.execute_reply.started":"2025-10-30T12:28:15.805958Z","shell.execute_reply":"2025-10-30T12:28:15.812543Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get a random sentence from the training set\nrandom_sentence = random.choice(train_sentences)\nprint(f\"original text: \\n{random_sentence} \")\nrandom_sentence_vectorized = text_vectorizer(random_sentence)\nsample_embed = embedding(random_sentence_vectorized)\nrandom_sentence_vectorized,sample_embed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:15.813998Z","iopub.execute_input":"2025-10-30T12:28:15.814258Z","iopub.status.idle":"2025-10-30T12:28:15.848902Z","shell.execute_reply.started":"2025-10-30T12:28:15.814222Z","shell.execute_reply":"2025-10-30T12:28:15.848302Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# check out a single token's embedding\nsample_embed[0], sample_embed[0].shape,random_sentence_vectorized[14]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:15.849620Z","iopub.execute_input":"2025-10-30T12:28:15.849821Z","iopub.status.idle":"2025-10-30T12:28:15.857221Z","shell.execute_reply.started":"2025-10-30T12:28:15.849806Z","shell.execute_reply":"2025-10-30T12:28:15.856454Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Modeling a text dataset (running a series of experiment)\nNow we've a got way to turn our text sequences into numbers, it's time to start building a series of modelling experiments.\nwe'll start with a baseline and move on from there.\n\n* Model 0: Naive Bayes(baseline)\n* Model 1: Feed-Forwared neural Network(dense Model)\n* Model 2: LSTM model(RNN)\n* Model 3: GRU model(RNN)\n* Model 4: Bidirectional-LSTM model(RNN)\n* Model 5:1D Convolutional Neural Network(CNN)\n* Model 6: TensorFlow Hub pretrained Feature Extractor(using transofer learning for NLP)\n* Model 7: same as model 6 with 10% of training data\n\n  How we are going to approach all of these?\n\n  Use the standard steps in modelling with tensorflow:\n  * Create a model\n  * Build a model\n  * Fit a model\n  * Evaluate a model\n  ","metadata":{}},{"cell_type":"markdown","source":"### Model 0: Getting a baseline \nAs with all machine learning modelling experiments, it's important to create a baseline model so you've got a benchmark for future experiment to build on","metadata":{}},{"cell_type":"code","source":"# !pip install scikit-learn\nimport sklearn\nprint(sklearn.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:15.857986Z","iopub.execute_input":"2025-10-30T12:28:15.858234Z","iopub.status.idle":"2025-10-30T12:28:15.869377Z","shell.execute_reply.started":"2025-10-30T12:28:15.858200Z","shell.execute_reply":"2025-10-30T12:28:15.868754Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:15.870112Z","iopub.execute_input":"2025-10-30T12:28:15.870296Z","iopub.status.idle":"2025-10-30T12:28:15.883947Z","shell.execute_reply.started":"2025-10-30T12:28:15.870281Z","shell.execute_reply":"2025-10-30T12:28:15.883077Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create tokenization and modelling pipeline \nmodel_0 = Pipeline([\n    (\"tfidf\",TfidfVectorizer()), # convert words into numbers using tfidf\n    (\"clf\",MultinomialNB()) # model the text\n])\n\n# Fit the pipeline to the training data\nmodel_0_history = model_0.fit(train_sentences,train_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:15.884872Z","iopub.execute_input":"2025-10-30T12:28:15.885171Z","iopub.status.idle":"2025-10-30T12:28:16.028804Z","shell.execute_reply.started":"2025-10-30T12:28:15.885145Z","shell.execute_reply":"2025-10-30T12:28:16.028171Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_0_history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:16.029688Z","iopub.execute_input":"2025-10-30T12:28:16.029956Z","iopub.status.idle":"2025-10-30T12:28:16.036891Z","shell.execute_reply.started":"2025-10-30T12:28:16.029932Z","shell.execute_reply":"2025-10-30T12:28:16.036117Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate our baseline model\nbaseline_score = model_0.score(val_sentences,val_labels)\nbaseline_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:16.037594Z","iopub.execute_input":"2025-10-30T12:28:16.037874Z","iopub.status.idle":"2025-10-30T12:28:16.067189Z","shell.execute_reply.started":"2025-10-30T12:28:16.037857Z","shell.execute_reply":"2025-10-30T12:28:16.066488Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Our baseline model achievs an accuracy of: {baseline_score}\")\ntrain_df.target.value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:16.068204Z","iopub.execute_input":"2025-10-30T12:28:16.068446Z","iopub.status.idle":"2025-10-30T12:28:16.079646Z","shell.execute_reply.started":"2025-10-30T12:28:16.068429Z","shell.execute_reply":"2025-10-30T12:28:16.078732Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# make predictions\nbaseline_preds = model_0.predict(val_sentences)\nbaseline_preds[:2],val_sentences[:2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:16.080533Z","iopub.execute_input":"2025-10-30T12:28:16.080953Z","iopub.status.idle":"2025-10-30T12:28:16.106617Z","shell.execute_reply.started":"2025-10-30T12:28:16.080929Z","shell.execute_reply":"2025-10-30T12:28:16.105735Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Creating an evaluation function for our model experiments\nwe could evaluate all of our model's predictions with different metrics every time,however, this will be cumbersome and could easily be fixed using function\nLet's create one to compare our model's predictions with the truth labels using the following metrics:\n* Accuracy\n* Percision\n* Recall\n* F1-score","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score,precision_recall_fscore_support\ndef calculate_results(y_true, y_preds):\n    \"\"\"\n    Calculates model accuracy, recall, precision and f1-score\n    of a binary classification model.\n    \"\"\"\n    model_accuracy = accuracy_score(y_true, y_preds) * 100\n    model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(\n        y_true, y_preds, average=\"weighted\"\n    )\n    model_results = {\n        \"accuracy\": model_accuracy,\n        \"precision\": model_precision,\n        \"recall\": model_recall,\n        \"f1\": model_f1,\n    }\n    return model_results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:16.107537Z","iopub.execute_input":"2025-10-30T12:28:16.108177Z","iopub.status.idle":"2025-10-30T12:28:16.117351Z","shell.execute_reply.started":"2025-10-30T12:28:16.108148Z","shell.execute_reply":"2025-10-30T12:28:16.116477Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"baseline_results = calculate_results(y_true=val_labels,y_preds=baseline_preds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:16.118266Z","iopub.execute_input":"2025-10-30T12:28:16.118636Z","iopub.status.idle":"2025-10-30T12:28:16.136006Z","shell.execute_reply.started":"2025-10-30T12:28:16.118611Z","shell.execute_reply":"2025-10-30T12:28:16.135088Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"baseline_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:16.137003Z","iopub.execute_input":"2025-10-30T12:28:16.137236Z","iopub.status.idle":"2025-10-30T12:28:16.142928Z","shell.execute_reply.started":"2025-10-30T12:28:16.137217Z","shell.execute_reply":"2025-10-30T12:28:16.142187Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model 1: A simple dense model","metadata":{}},{"cell_type":"code","source":"# Create a directory to save tensorboard logs\nSAVE_DIR = \"model_logs\"\n# Bild model with the Functional API\nfrom tensorflow.keras import layers\ninputs = layers.Input(shape=(),dtype=tf.string,name=\"input_layer\")\nx = text_vectorizer(inputs) # turn the input text into number\nx = embedding(x) # create an embedding of the numberized inputs\nx = layers.GlobalAveragePooling1D(name=\"globalAverage_layer\")(x)\n# x = layers.Flatten()(x)\noutputs = layers.Dense(1,activation=\"sigmoid\",name=\"output_layer\")(x)\nmodel_1 = tf.keras.Model(inputs,outputs,name=\"model_1_dense\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:16.143763Z","iopub.execute_input":"2025-10-30T12:28:16.143985Z","iopub.status.idle":"2025-10-30T12:28:16.167638Z","shell.execute_reply.started":"2025-10-30T12:28:16.143962Z","shell.execute_reply":"2025-10-30T12:28:16.166994Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_1.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:16.168277Z","iopub.execute_input":"2025-10-30T12:28:16.168447Z","iopub.status.idle":"2025-10-30T12:28:16.184733Z","shell.execute_reply.started":"2025-10-30T12:28:16.168433Z","shell.execute_reply":"2025-10-30T12:28:16.183988Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# compile the model\nmodel_1.compile(loss=\"binary_crossentropy\",\n               optimizer=tf.keras.optimizers.Adam(),\n               metrics=[\"accuracy\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:16.185407Z","iopub.execute_input":"2025-10-30T12:28:16.185620Z","iopub.status.idle":"2025-10-30T12:28:16.194407Z","shell.execute_reply.started":"2025-10-30T12:28:16.185603Z","shell.execute_reply":"2025-10-30T12:28:16.193677Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fit the model\nmodel_1_history = model_1.fit(x=train_sentences,\n                             y=train_labels,\n                             epochs=5,\n                             validation_data=(val_sentences,val_labels),\n                             callbacks=[create_tensorboard_callback(\"TensorBoard\",\"model_1_dense\")])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:16.195287Z","iopub.execute_input":"2025-10-30T12:28:16.195547Z","iopub.status.idle":"2025-10-30T12:28:24.430861Z","shell.execute_reply.started":"2025-10-30T12:28:16.195524Z","shell.execute_reply":"2025-10-30T12:28:24.430244Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_loss_curves(model_1_history)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:24.432209Z","iopub.execute_input":"2025-10-30T12:28:24.432585Z","iopub.status.idle":"2025-10-30T12:28:24.880231Z","shell.execute_reply.started":"2025-10-30T12:28:24.432557Z","shell.execute_reply":"2025-10-30T12:28:24.879390Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_1.evaluate(val_sentences,val_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:24.881157Z","iopub.execute_input":"2025-10-30T12:28:24.881447Z","iopub.status.idle":"2025-10-30T12:28:25.060427Z","shell.execute_reply.started":"2025-10-30T12:28:24.881429Z","shell.execute_reply":"2025-10-30T12:28:25.059839Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_1_pred_probs = model_1.predict(val_sentences)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:25.061204Z","iopub.execute_input":"2025-10-30T12:28:25.061464Z","iopub.status.idle":"2025-10-30T12:28:25.516297Z","shell.execute_reply.started":"2025-10-30T12:28:25.061436Z","shell.execute_reply":"2025-10-30T12:28:25.515699Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_1_pred_probs.shape,val_sentences[:5],model_1_pred_probs[:5]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:25.517306Z","iopub.execute_input":"2025-10-30T12:28:25.517602Z","iopub.status.idle":"2025-10-30T12:28:25.523523Z","shell.execute_reply.started":"2025-10-30T12:28:25.517578Z","shell.execute_reply":"2025-10-30T12:28:25.522699Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# convert model prediction probablity to a label format\nmodel_1_preds = tf.squeeze(tf.round(model_1_pred_probs))\nmodel_1_preds[:20]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:25.524283Z","iopub.execute_input":"2025-10-30T12:28:25.524570Z","iopub.status.idle":"2025-10-30T12:28:25.538249Z","shell.execute_reply.started":"2025-10-30T12:28:25.524547Z","shell.execute_reply":"2025-10-30T12:28:25.537572Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_1_results = calculate_results(y_true=val_labels,y_preds=model_1_preds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:25.538941Z","iopub.execute_input":"2025-10-30T12:28:25.539177Z","iopub.status.idle":"2025-10-30T12:28:25.554380Z","shell.execute_reply.started":"2025-10-30T12:28:25.539159Z","shell.execute_reply":"2025-10-30T12:28:25.553743Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_1_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:25.555180Z","iopub.execute_input":"2025-10-30T12:28:25.555427Z","iopub.status.idle":"2025-10-30T12:28:25.562149Z","shell.execute_reply.started":"2025-10-30T12:28:25.555407Z","shell.execute_reply":"2025-10-30T12:28:25.561567Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"baseline_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:25.562855Z","iopub.execute_input":"2025-10-30T12:28:25.563069Z","iopub.status.idle":"2025-10-30T12:28:25.577443Z","shell.execute_reply.started":"2025-10-30T12:28:25.563052Z","shell.execute_reply":"2025-10-30T12:28:25.576708Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nnp.array(list(model_1_results.values())) > np.array(list(baseline_results.values()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:25.578218Z","iopub.execute_input":"2025-10-30T12:28:25.578418Z","iopub.status.idle":"2025-10-30T12:28:25.593130Z","shell.execute_reply.started":"2025-10-30T12:28:25.578399Z","shell.execute_reply":"2025-10-30T12:28:25.592556Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualizing Learned Embeddings","metadata":{}},{"cell_type":"code","source":"# Get the vocabulary from the text vectorization layer\nwords_in_vocab = text_vectorizer.get_vocabulary()\nlen(words_in_vocab), words_in_vocab[:10]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:25.593779Z","iopub.execute_input":"2025-10-30T12:28:25.594026Z","iopub.status.idle":"2025-10-30T12:28:25.631953Z","shell.execute_reply.started":"2025-10-30T12:28:25.594011Z","shell.execute_reply":"2025-10-30T12:28:25.631299Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model 1 summary\nmodel_1.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:25.632756Z","iopub.execute_input":"2025-10-30T12:28:25.633111Z","iopub.status.idle":"2025-10-30T12:28:25.650130Z","shell.execute_reply.started":"2025-10-30T12:28:25.633086Z","shell.execute_reply":"2025-10-30T12:28:25.649338Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# get the weight metrics of embedding layer\n# (these are the numberical representations of each token in our training data, which have been learned for 5 epochs)\nprint(f\"total parameter of embedding weights: {len(embedding.get_weights()[0])}\")\nembed_weights = model_1.get_layer(\"embedding_1\").get_weights()[0]\nprint(embed_weights[:20])\nprint(f\"shape of embedding: {embed_weights.shape} \")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:29:11.942226Z","iopub.execute_input":"2025-10-30T12:29:11.942799Z","iopub.status.idle":"2025-10-30T12:29:11.954461Z","shell.execute_reply.started":"2025-10-30T12:29:11.942770Z","shell.execute_reply":"2025-10-30T12:29:11.953567Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we've got the embedding matrix our mdoel has learned to represent our tokens, let's see how we can visualize it.\nto do so, TensorFlow as a handy tool called projector.\nand tensorflow alos has an incredible guide on word embeddings themselves.","metadata":{}},{"cell_type":"code","source":"words_in_vocab[:10]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:29:17.599591Z","iopub.execute_input":"2025-10-30T12:29:17.600186Z","iopub.status.idle":"2025-10-30T12:29:17.605177Z","shell.execute_reply.started":"2025-10-30T12:29:17.600162Z","shell.execute_reply":"2025-10-30T12:29:17.604535Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create embedding files from tensorflow word embedding docs\nimport io\nout_v = io.open('vectors.tsv', 'w', encoding='utf-8')\nout_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n\nfor index, word in enumerate(words_in_vocab):\n  if index == 0:\n    continue  # skip 0, it's padding.\n  vec = embed_weights[index]\n  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n  out_m.write(word + \"\\n\")\nout_v.close()\nout_m.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:29:25.422080Z","iopub.execute_input":"2025-10-30T12:29:25.422811Z","iopub.status.idle":"2025-10-30T12:29:26.162404Z","shell.execute_reply.started":"2025-10-30T12:29:25.422785Z","shell.execute_reply":"2025-10-30T12:29:26.161577Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorboard\nprint(tensorboard.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:29:26.435206Z","iopub.execute_input":"2025-10-30T12:29:26.435835Z","iopub.status.idle":"2025-10-30T12:29:26.440131Z","shell.execute_reply.started":"2025-10-30T12:29:26.435806Z","shell.execute_reply":"2025-10-30T12:29:26.439496Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#docs_infra: no_execute\n%load_ext tensorboard\n%tensorboard --logdir /kaggle/working/TensorBoard/model_1_dense","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:29:29.815961Z","iopub.execute_input":"2025-10-30T12:29:29.816621Z","iopub.status.idle":"2025-10-30T12:29:29.826340Z","shell.execute_reply.started":"2025-10-30T12:29:29.816596Z","shell.execute_reply":"2025-10-30T12:29:29.825674Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Recurrent Neural Networks (RNN's)\nRNN's are useful sequence data.\n\nThe promise of a recurrent neural network is to use the ***`representation`*** of a previous input to aid the representation of a later input.","metadata":{}},{"cell_type":"markdown","source":"### model 2: LSTM\nLSTM = long short-term memory (one of the most popular LSTM cells)\nOUr structure of an RNN typically looks like this:\n```\nInput(text) -> Tokenize -> Embedding -> Layers(RNNs/Dense) -> Output(label probability)\n```","metadata":{}},{"cell_type":"code","source":"# Create an LSTM model\nfrom tensorflow.keras import layers\ninputs = layers.Input(shape=(1,),dtype=\"string\")\nx = text_vectorizer(inputs)\n# print(x.shape)\nx = embedding(x)\n# print(x.shape)\n# x = layers.LSTM(units=64,return_sequences=True)(x)\n# print(x.shape)\nx = layers.LSTM(64)(x)\n# print(x.shape)\n# x = layers.Dense(64,activation=\"relu\")(x)\noutputs = layers.Dense(1,activation=\"sigmoid\")(x)\nmodel_2 = tf.keras.Model(inputs,outputs,name=\"model_2_LSTM\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:29:37.058074Z","iopub.execute_input":"2025-10-30T12:29:37.058321Z","iopub.status.idle":"2025-10-30T12:29:37.090307Z","shell.execute_reply.started":"2025-10-30T12:29:37.058305Z","shell.execute_reply":"2025-10-30T12:29:37.089719Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_2.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:29:39.492932Z","iopub.execute_input":"2025-10-30T12:29:39.493491Z","iopub.status.idle":"2025-10-30T12:29:39.510126Z","shell.execute_reply.started":"2025-10-30T12:29:39.493464Z","shell.execute_reply":"2025-10-30T12:29:39.509484Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_2.compile(loss=\"binary_crossentropy\",\n               optimizer=tf.keras.optimizers.Adam(),\n               metrics=[\"accuracy\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:29:43.642746Z","iopub.execute_input":"2025-10-30T12:29:43.643262Z","iopub.status.idle":"2025-10-30T12:29:43.651924Z","shell.execute_reply.started":"2025-10-30T12:29:43.643239Z","shell.execute_reply":"2025-10-30T12:29:43.651319Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_2_history = model_2.fit(train_sentences,\n                             train_labels,\n                             epochs=5,\n                             validation_data=(val_sentences,val_labels),\n                             callbacks=[create_tensorboard_callback(SAVE_DIR,\"model_2_LSTM\")])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:29:45.848656Z","iopub.execute_input":"2025-10-30T12:29:45.848931Z","iopub.status.idle":"2025-10-30T12:29:57.172488Z","shell.execute_reply.started":"2025-10-30T12:29:45.848912Z","shell.execute_reply":"2025-10-30T12:29:57.171888Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# make prediction with LSTM model\nmodel_2_pred_probs = model_2.predict(val_sentences)\nmodel_2_pred_probs[:10]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:29:57.174219Z","iopub.execute_input":"2025-10-30T12:29:57.174490Z","iopub.status.idle":"2025-10-30T12:29:57.858157Z","shell.execute_reply.started":"2025-10-30T12:29:57.174473Z","shell.execute_reply":"2025-10-30T12:29:57.857361Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_2_preds = tf.squeeze(tf.round(model_2_pred_probs))\nmodel_2_preds[:10]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:29:57.859403Z","iopub.execute_input":"2025-10-30T12:29:57.859655Z","iopub.status.idle":"2025-10-30T12:29:57.867389Z","shell.execute_reply.started":"2025-10-30T12:29:57.859637Z","shell.execute_reply":"2025-10-30T12:29:57.866574Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_labels[:10]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:30:01.851880Z","iopub.execute_input":"2025-10-30T12:30:01.852460Z","iopub.status.idle":"2025-10-30T12:30:01.857383Z","shell.execute_reply.started":"2025-10-30T12:30:01.852426Z","shell.execute_reply":"2025-10-30T12:30:01.856566Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate model 2 results\nmodel_2_results = calculate_results(y_true=val_labels,y_preds=model_2_preds)\nmodel_2_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:30:04.417692Z","iopub.execute_input":"2025-10-30T12:30:04.418218Z","iopub.status.idle":"2025-10-30T12:30:04.426826Z","shell.execute_reply.started":"2025-10-30T12:30:04.418197Z","shell.execute_reply":"2025-10-30T12:30:04.426038Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"baseline_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:30:07.348249Z","iopub.execute_input":"2025-10-30T12:30:07.348989Z","iopub.status.idle":"2025-10-30T12:30:07.353569Z","shell.execute_reply.started":"2025-10-30T12:30:07.348963Z","shell.execute_reply":"2025-10-30T12:30:07.352900Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from math import e\nprint(e)\ndef tanh_func(z):\n    a = (e**z - e**(-z))/(e**z + e**(-z)) # tensorflow don't know math and python label code durring computational graph, it must be tensorflow optrational for derivation of gradients\n    return a\n    # return (tf.exp(z) - tf.exp(-z)) / (tf.exp(z) + tf.exp(-z))\n    # return tf.math.tanh(z)\ntanh_func(1.)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:30:09.259291Z","iopub.execute_input":"2025-10-30T12:30:09.259565Z","iopub.status.idle":"2025-10-30T12:30:09.266408Z","shell.execute_reply.started":"2025-10-30T12:30:09.259546Z","shell.execute_reply":"2025-10-30T12:30:09.265775Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### build GRU model 3\nAnother popular and effective RNN component is the GRU or gated recurrent unit.\nThe GRU cell has similar features to an LSTM cell but has less parameters.","metadata":{}},{"cell_type":"code","source":"inputs = layers.Input(shape=(1,),dtype=tf.string,name=\"input_layer\")\nx = text_vectorizer(inputs)\nx = embedding(x)\nx = layers.GRU(units=64,activation=tanh_func)(x)\n# x = layers.Lambda(tanh_func)(x)\noutputs = layers.Dense(1,activation=\"sigmoid\",name=\"output_layer\")(x)\nmodel_3 = tf.keras.Model(inputs,outputs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:30:12.784211Z","iopub.execute_input":"2025-10-30T12:30:12.784477Z","iopub.status.idle":"2025-10-30T12:30:12.815604Z","shell.execute_reply.started":"2025-10-30T12:30:12.784461Z","shell.execute_reply":"2025-10-30T12:30:12.814837Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_3.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:28:25.709682Z","iopub.status.idle":"2025-10-30T12:28:25.709953Z","shell.execute_reply.started":"2025-10-30T12:28:25.709814Z","shell.execute_reply":"2025-10-30T12:28:25.709828Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# compile the model\nmodel_3.compile(loss=\"binary_crossentropy\",\n              optimizer=tf.keras.optimizers.Adam(),\n              metrics=[\"accuracy\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:30:17.512217Z","iopub.execute_input":"2025-10-30T12:30:17.512517Z","iopub.status.idle":"2025-10-30T12:30:17.521608Z","shell.execute_reply.started":"2025-10-30T12:30:17.512478Z","shell.execute_reply":"2025-10-30T12:30:17.520784Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_3_history = model_3.fit(train_sentences,\n                             train_labels,\n                             epochs=5,\n                             validation_data=(val_sentences,val_labels),\n                             callbacks=[create_tensorboard_callback(SAVE_DIR,\"model_3_GRU\")])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:30:20.653077Z","iopub.execute_input":"2025-10-30T12:30:20.653385Z","iopub.status.idle":"2025-10-30T12:30:59.408045Z","shell.execute_reply.started":"2025-10-30T12:30:20.653364Z","shell.execute_reply":"2025-10-30T12:30:59.407411Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_loss_curves(model_3_history)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:50:42.080244Z","iopub.execute_input":"2025-10-30T12:50:42.080890Z","iopub.status.idle":"2025-10-30T12:50:42.529463Z","shell.execute_reply.started":"2025-10-30T12:50:42.080861Z","shell.execute_reply":"2025-10-30T12:50:42.528636Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_3_pred_probs = model_3.predict(val_sentences)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:30:59.409936Z","iopub.execute_input":"2025-10-30T12:30:59.410206Z","iopub.status.idle":"2025-10-30T12:31:00.624039Z","shell.execute_reply.started":"2025-10-30T12:30:59.410191Z","shell.execute_reply":"2025-10-30T12:31:00.623337Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_3_preds = tf.squeeze(tf.round(model_3_pred_probs))\nmodel_3_preds[:10]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:31:00.625307Z","iopub.execute_input":"2025-10-30T12:31:00.625631Z","iopub.status.idle":"2025-10-30T12:31:00.632686Z","shell.execute_reply.started":"2025-10-30T12:31:00.625613Z","shell.execute_reply":"2025-10-30T12:31:00.632054Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_3_results = calculate_results(y_true=val_labels,y_preds=model_3_preds)\nmodel_3_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:31:00.634143Z","iopub.execute_input":"2025-10-30T12:31:00.634349Z","iopub.status.idle":"2025-10-30T12:31:00.649158Z","shell.execute_reply.started":"2025-10-30T12:31:00.634333Z","shell.execute_reply":"2025-10-30T12:31:00.648551Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model 4: Bidirectional RNN\nNormal RNN go from left to right (just like you'd read and English sentence), however, a bidirectional RNN goes from right to left as well as left to right.\n","metadata":{}},{"cell_type":"code","source":"# Build a bidirectional RNN in tensorflow\nimport tensorflow as tf\nfrom tensorflow.keras import layers\ninputs = layers.Input(shape=(1,),dtype=tf.string)\nx = text_vectorizer(inputs)\nprint(x.shape)\nx = embedding(x)\nprint(x.shape)\n# x = layers.Bidirectional(layers.LSTM(64,return_sequences=True))(x)\n# print(x.shape)\nx = layers.Bidirectional(layers.LSTM(64))(x)\nprint(x.shape)\noutputs = layers.Dense(1,activation=\"sigmoid\")(x)\nmodel_4 = tf.keras.Model(inputs,outputs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:46:07.314182Z","iopub.execute_input":"2025-10-30T12:46:07.314475Z","iopub.status.idle":"2025-10-30T12:46:07.366141Z","shell.execute_reply.started":"2025-10-30T12:46:07.314455Z","shell.execute_reply":"2025-10-30T12:46:07.365440Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_4.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:46:11.137903Z","iopub.execute_input":"2025-10-30T12:46:11.138550Z","iopub.status.idle":"2025-10-30T12:46:11.154080Z","shell.execute_reply.started":"2025-10-30T12:46:11.138495Z","shell.execute_reply":"2025-10-30T12:46:11.153453Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_4.compile(loss=\"binary_crossentropy\",\n              optimizer=tf.keras.optimizers.Adam(),\n              metrics=[\"accuracy\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:46:53.080728Z","iopub.execute_input":"2025-10-30T12:46:53.080985Z","iopub.status.idle":"2025-10-30T12:46:53.090365Z","shell.execute_reply.started":"2025-10-30T12:46:53.080969Z","shell.execute_reply":"2025-10-30T12:46:53.089405Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_4_history = model_4.fit(train_sentences,\n                             train_labels,\n                             epochs=5,\n                             validation_data=(val_sentences,val_labels),\n                             callbacks=[create_tensorboard_callback(SAVE_DIR,\"model_4_bidirectional\")])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:49:07.546577Z","iopub.execute_input":"2025-10-30T12:49:07.547310Z","iopub.status.idle":"2025-10-30T12:49:20.498478Z","shell.execute_reply.started":"2025-10-30T12:49:07.547286Z","shell.execute_reply":"2025-10-30T12:49:20.497931Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_loss_curves(model_4_history)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:49:51.950373Z","iopub.execute_input":"2025-10-30T12:49:51.951133Z","iopub.status.idle":"2025-10-30T12:49:52.377370Z","shell.execute_reply.started":"2025-10-30T12:49:51.951105Z","shell.execute_reply":"2025-10-30T12:49:52.376104Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_4_pred_probs = model_4.predict(val_sentences)\nmodel_4_pred_probs[:10]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:51:30.136868Z","iopub.execute_input":"2025-10-30T12:51:30.137117Z","iopub.status.idle":"2025-10-30T12:51:31.732091Z","shell.execute_reply.started":"2025-10-30T12:51:30.137100Z","shell.execute_reply":"2025-10-30T12:51:31.731472Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_4_preds = tf.squeeze(tf.round(model_4_pred_probs))\nmodel_4_preds[:10]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:52:17.917794Z","iopub.execute_input":"2025-10-30T12:52:17.918084Z","iopub.status.idle":"2025-10-30T12:52:17.925407Z","shell.execute_reply.started":"2025-10-30T12:52:17.918061Z","shell.execute_reply":"2025-10-30T12:52:17.924782Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_4_results = calculate_results(y_true=val_labels,y_preds=model_4_preds)\nmodel_4_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T12:53:29.005049Z","iopub.execute_input":"2025-10-30T12:53:29.005605Z","iopub.status.idle":"2025-10-30T12:53:29.014095Z","shell.execute_reply.started":"2025-10-30T12:53:29.005570Z","shell.execute_reply":"2025-10-30T12:53:29.013378Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}