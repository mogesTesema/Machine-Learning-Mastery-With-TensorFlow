{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM1RWUqbK7bz/UPq9Ifbz5f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mogesTesema/Machine-Learning-Mastery-With-TensorFlow/blob/main/05_transfer_learning_in_tensorflow_part_2_Fine_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning with tensorflow part 2: Fine-Tuning"
      ],
      "metadata": {
        "id": "oh0cuiUGqCKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating helper functions"
      ],
      "metadata": {
        "id": "DmsXYfJ_qzvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/refs/heads/main/extras/helper_functions.py"
      ],
      "metadata": {
        "id": "89PnvMZRq33F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e50c2a8d-2929-401a-9973-35d067d1de7f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-18 14:50:46--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/refs/heads/main/extras/helper_functions.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10246 (10K) [text/plain]\n",
            "Saving to: ‘helper_functions.py’\n",
            "\n",
            "\rhelper_functions.py   0%[                    ]       0  --.-KB/s               \rhelper_functions.py 100%[===================>]  10.01K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-10-18 14:50:47 (94.3 MB/s) - ‘helper_functions.py’ saved [10246/10246]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import plot_loss_curves,create_tensorboard_callback,confusion_matrix,load_and_prep_image,walk_through_dir,unzip_data"
      ],
      "metadata": {
        "id": "OIaLuXLdvEZ3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's get some data"
      ],
      "metadata": {
        "id": "fISBoVDqvWEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip"
      ],
      "metadata": {
        "id": "zq4zI8mk0_QW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20ba6dbb-99a5-4a8d-cbbc-296bbab330bb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-18 14:50:52--  https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.152.207, 142.250.125.207, 192.178.210.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.152.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 168546183 (161M) [application/zip]\n",
            "Saving to: ‘10_food_classes_10_percent.zip’\n",
            "\n",
            "10_food_classes_10_ 100%[===================>] 160.74M   239MB/s    in 0.7s    \n",
            "\n",
            "2025-10-18 14:50:53 (239 MB/s) - ‘10_food_classes_10_percent.zip’ saved [168546183/168546183]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unzip_data('10_food_classes_10_percent.zip')"
      ],
      "metadata": {
        "id": "NOunwkkc16Ls"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "walk_through_dir(\"10_food_classes_10_percent\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icAJIpmQ2JwO",
        "outputId": "5b49237d-e0d1-4374-ddec-b459b7a98095"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 directories and 0 images in '10_food_classes_10_percent'.\n",
            "There are 10 directories and 0 images in '10_food_classes_10_percent/test'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/pizza'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/fried_rice'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_curry'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/grilled_salmon'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/hamburger'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/ramen'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/steak'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_wings'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/ice_cream'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/sushi'.\n",
            "There are 10 directories and 0 images in '10_food_classes_10_percent/train'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/pizza'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/fried_rice'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_curry'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/grilled_salmon'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/hamburger'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/ramen'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/steak'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_wings'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/ice_cream'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/sushi'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and test directory paths\n",
        "train_dir = \"/content/10_food_classes_10_percent/train\"\n",
        "test_dir = \"/content/10_food_classes_10_percent/test\"\n"
      ],
      "metadata": {
        "id": "q0g2rwRT2en5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "eo86aeWb3RFg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = (224,224)\n",
        "BATCH_SIZE = 32\n",
        "train_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(directory=train_dir,\n",
        "                                                                            image_size=IMG_SIZE,\n",
        "                                                                            label_mode='categorical',\n",
        "                                                                            batch_size=BATCH_SIZE\n",
        "                                                                            )\n",
        "test_data = tf.keras.preprocessing.image_dataset_from_directory(directory=test_dir,\n",
        "                                                                image_size=IMG_SIZE,\n",
        "                                                                batch_size=BATCH_SIZE,\n",
        "                                                                label_mode=\"categorical\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UALLS4q04BCl",
        "outputId": "6aa24632-a236-422b-e61a-6b0144190cf1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 750 files belonging to 10 classes.\n",
            "Found 2500 files belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_10_percent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A60u9qB14AHD",
        "outputId": "bd0fe85e-b666-4c20-a00f-badb6f6d842d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 10), dtype=tf.float32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_10_percent.class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2yfVCPP9ubu",
        "outputId": "f1f9cd2e-1024-4479-836f-5c14aabaa2de"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['chicken_curry',\n",
              " 'chicken_wings',\n",
              " 'fried_rice',\n",
              " 'grilled_salmon',\n",
              " 'hamburger',\n",
              " 'ice_cream',\n",
              " 'pizza',\n",
              " 'ramen',\n",
              " 'steak',\n",
              " 'sushi']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for images,labels in train_data_10_percent.take(1):\n",
        "  print(images,labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_3sKq23-WZX",
        "outputId": "e246fcc7-63fc-4824-fbad-2a17aea9df1b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[[1.49056122e+02 1.43056122e+02 1.31056122e+02]\n",
            "   [1.56000015e+02 1.50000015e+02 1.38000015e+02]\n",
            "   [1.60290817e+02 1.54290817e+02 1.40852036e+02]\n",
            "   ...\n",
            "   [1.30566330e+02 1.25566338e+02 1.03566338e+02]\n",
            "   [1.31571411e+02 1.26571419e+02 1.06571419e+02]\n",
            "   [1.30285721e+02 1.25285713e+02 1.05285713e+02]]\n",
            "\n",
            "  [[1.55357147e+02 1.52357147e+02 1.37357147e+02]\n",
            "   [1.59637756e+02 1.56637756e+02 1.41637756e+02]\n",
            "   [1.56515305e+02 1.53515305e+02 1.38117355e+02]\n",
            "   ...\n",
            "   [1.27341812e+02 1.25341812e+02 1.02341812e+02]\n",
            "   [1.27209198e+02 1.25209198e+02 1.02209198e+02]\n",
            "   [1.29403091e+02 1.27403091e+02 1.04403091e+02]]\n",
            "\n",
            "  [[1.61290817e+02 1.58290817e+02 1.40862244e+02]\n",
            "   [1.57586731e+02 1.54586731e+02 1.37158157e+02]\n",
            "   [1.50188782e+02 1.47188782e+02 1.29760208e+02]\n",
            "   ...\n",
            "   [1.28954086e+02 1.26954086e+02 1.04382660e+02]\n",
            "   [1.30285721e+02 1.28285721e+02 1.05714302e+02]\n",
            "   [1.32000000e+02 1.30000000e+02 1.07428574e+02]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[2.75815506e+01 7.64283514e+00 1.65300179e+00]\n",
            "   [2.91428146e+01 9.71428680e+00 3.92855072e+00]\n",
            "   [3.07805557e+01 1.05408630e+01 7.13780689e+00]\n",
            "   ...\n",
            "   [4.59641533e+01 2.37957993e+01 3.46921968e+00]\n",
            "   [3.40154724e+01 1.60308418e+01 1.69409680e+00]\n",
            "   [1.16879501e+02 1.02736771e+02 8.33080292e+01]]\n",
            "\n",
            "  [[3.40000916e+01 1.45970211e+01 7.59702158e+00]\n",
            "   [2.70153084e+01 8.00510311e+00 2.01020598e+00]\n",
            "   [3.01428833e+01 1.27398138e+01 9.11229229e+00]\n",
            "   ...\n",
            "   [5.54946289e+01 3.34946289e+01 1.02803659e+01]\n",
            "   [8.08073425e+01 6.30931129e+01 3.89451256e+01]\n",
            "   [1.46443771e+02 1.33183640e+02 1.07086685e+02]]\n",
            "\n",
            "  [[4.73878708e+01 2.56275978e+01 1.80408401e+01]\n",
            "   [2.94031620e+01 9.59697533e+00 4.00006866e+00]\n",
            "   [3.29898148e+01 1.44285364e+01 1.31377478e+01]\n",
            "   ...\n",
            "   [8.62762756e+01 6.42762756e+01 4.02762794e+01]\n",
            "   [1.46449051e+02 1.29663376e+02 1.02785736e+02]\n",
            "   [1.50066422e+02 1.37137955e+02 1.07780769e+02]]]\n",
            "\n",
            "\n",
            " [[[1.59188782e+02 1.58775513e+02 1.54801025e+02]\n",
            "   [1.57586731e+02 1.54637741e+02 1.48469391e+02]\n",
            "   [1.67994904e+02 1.59051025e+02 1.54566315e+02]\n",
            "   ...\n",
            "   [4.53163681e+01 5.15408630e+01 1.86072197e+01]\n",
            "   [4.08724632e+01 4.98928757e+01 2.31327171e+01]\n",
            "   [5.34489746e+01 6.24489746e+01 4.41633301e+01]]\n",
            "\n",
            "  [[1.68107147e+02 1.61392853e+02 1.53515305e+02]\n",
            "   [1.62852036e+02 1.54872452e+02 1.44081619e+02]\n",
            "   [1.74153061e+02 1.60010193e+02 1.51954086e+02]\n",
            "   ...\n",
            "   [4.73878174e+01 5.65613480e+01 2.18317375e+01]\n",
            "   [5.13826218e+01 6.23775444e+01 3.27347374e+01]\n",
            "   [5.55459518e+01 6.74082031e+01 4.61684723e+01]]\n",
            "\n",
            "  [[1.62668365e+02 1.53020401e+02 1.42959183e+02]\n",
            "   [1.68423477e+02 1.56198975e+02 1.44994904e+02]\n",
            "   [1.74678574e+02 1.57938782e+02 1.51010208e+02]\n",
            "   ...\n",
            "   [5.70255165e+01 6.84286194e+01 3.30255356e+01]\n",
            "   [5.47142487e+01 6.77856903e+01 3.80714569e+01]\n",
            "   [5.40766449e+01 6.88011856e+01 4.48675728e+01]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[1.66770538e+02 1.91556274e+02 1.95556274e+02]\n",
            "   [1.65443924e+02 1.90413315e+02 1.97285767e+02]\n",
            "   [1.68974548e+02 1.94331757e+02 2.08311340e+02]\n",
            "   ...\n",
            "   [1.51831650e+02 1.65403122e+02 1.65403122e+02]\n",
            "   [1.49953949e+02 1.63683624e+02 1.63183655e+02]\n",
            "   [1.46362503e+02 1.61229950e+02 1.58367691e+02]]\n",
            "\n",
            "  [[1.63948929e+02 1.89086700e+02 1.94040771e+02]\n",
            "   [1.65938782e+02 1.93005112e+02 2.02005142e+02]\n",
            "   [1.61617325e+02 1.88744904e+02 2.06545898e+02]\n",
            "   ...\n",
            "   [1.42198898e+02 1.58198898e+02 1.58198898e+02]\n",
            "   [1.36872421e+02 1.53290863e+02 1.52153091e+02]\n",
            "   [1.41872406e+02 1.61275513e+02 1.59275513e+02]]\n",
            "\n",
            "  [[1.61112228e+02 1.89112228e+02 1.93112228e+02]\n",
            "   [1.63071213e+02 1.91050812e+02 2.01882446e+02]\n",
            "   [1.55989532e+02 1.85004868e+02 2.03785477e+02]\n",
            "   ...\n",
            "   [1.38275467e+02 1.55846939e+02 1.55846939e+02]\n",
            "   [1.38357208e+02 1.57760315e+02 1.55760315e+02]\n",
            "   [1.36188080e+02 1.57361633e+02 1.54774857e+02]]]\n",
            "\n",
            "\n",
            " [[[2.65867348e+01 8.58673477e+00 0.00000000e+00]\n",
            "   [2.73571434e+01 9.35714245e+00 0.00000000e+00]\n",
            "   [2.83571434e+01 1.03571424e+01 0.00000000e+00]\n",
            "   ...\n",
            "   [2.02413269e+02 2.11561234e+02 2.21275528e+02]\n",
            "   [2.06637741e+02 2.16637741e+02 2.25994888e+02]\n",
            "   [2.04928574e+02 2.14928574e+02 2.24285721e+02]]\n",
            "\n",
            "  [[2.90714283e+01 1.10714283e+01 0.00000000e+00]\n",
            "   [2.99336739e+01 1.19336739e+01 0.00000000e+00]\n",
            "   [3.00000000e+01 1.20000000e+01 0.00000000e+00]\n",
            "   ...\n",
            "   [2.02989883e+02 2.11387802e+02 2.22188843e+02]\n",
            "   [2.03367310e+02 2.11367310e+02 2.22367310e+02]\n",
            "   [2.02316437e+02 2.10316437e+02 2.21316437e+02]]\n",
            "\n",
            "  [[3.00000000e+01 1.20000000e+01 0.00000000e+00]\n",
            "   [2.92704086e+01 1.12704077e+01 0.00000000e+00]\n",
            "   [2.92142868e+01 1.12142859e+01 0.00000000e+00]\n",
            "   ...\n",
            "   [2.02571426e+02 2.11357147e+02 2.20785721e+02]\n",
            "   [1.97928558e+02 2.04357132e+02 2.16142838e+02]\n",
            "   [2.05780884e+02 2.12209457e+02 2.23995163e+02]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[2.42858887e+00 0.00000000e+00 1.00000000e+00]\n",
            "   [3.42858887e+00 1.00000000e+00 2.00000000e+00]\n",
            "   [2.00000000e+00 0.00000000e+00 1.00000000e+00]\n",
            "   ...\n",
            "   [1.06719337e+02 1.15719337e+02 1.24719337e+02]\n",
            "   [1.02357147e+02 1.11357147e+02 1.20357147e+02]\n",
            "   [1.00352051e+02 1.09352051e+02 1.18352051e+02]]\n",
            "\n",
            "  [[4.00000000e+00 0.00000000e+00 1.00000000e+00]\n",
            "   [5.00000000e+00 1.00000000e+00 2.00000000e+00]\n",
            "   [3.45921087e+00 0.00000000e+00 1.00000000e+00]\n",
            "   ...\n",
            "   [1.05142822e+02 1.14142822e+02 1.23142822e+02]\n",
            "   [1.03928558e+02 1.12928558e+02 1.21928558e+02]\n",
            "   [1.01642822e+02 1.10642822e+02 1.19642822e+02]]\n",
            "\n",
            "  [[4.64285278e+00 0.00000000e+00 1.00000000e+00]\n",
            "   [5.00000000e+00 1.00000000e+00 2.00000000e+00]\n",
            "   [4.00000000e+00 0.00000000e+00 1.00000000e+00]\n",
            "   ...\n",
            "   [9.94234467e+01 1.08423447e+02 1.17423447e+02]\n",
            "   [9.67602234e+01 1.05760223e+02 1.14760223e+02]\n",
            "   [9.73010559e+01 1.06301056e+02 1.15301056e+02]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[1.58382660e+02 9.43826523e+01 0.00000000e+00]\n",
            "   [1.61045914e+02 9.89540863e+01 6.88775539e-01]\n",
            "   [1.63280609e+02 1.01280617e+02 2.28061271e+00]\n",
            "   ...\n",
            "   [2.38714279e+02 1.73142807e+02 1.67142849e+01]\n",
            "   [2.34571396e+02 1.64571396e+02 1.45714025e+01]\n",
            "   [2.28158051e+02 1.58158051e+02 1.01580458e+01]]\n",
            "\n",
            "  [[1.65265305e+02 9.93571396e+01 2.38775611e+00]\n",
            "   [1.60346939e+02 9.62142868e+01 2.04080895e-01]\n",
            "   [1.65755112e+02 1.01785721e+02 3.77040911e+00]\n",
            "   ...\n",
            "   [2.35270386e+02 1.69698914e+02 1.32703876e+01]\n",
            "   [2.31994904e+02 1.61994904e+02 1.19948969e+01]\n",
            "   [2.30739700e+02 1.60739700e+02 1.27396946e+01]]\n",
            "\n",
            "  [[1.64734695e+02 9.63061218e+01 3.06122601e-01]\n",
            "   [1.63683670e+02 9.56530609e+01 1.98979780e-01]\n",
            "   [1.63244904e+02 9.69081650e+01 6.73469663e-01]\n",
            "   ...\n",
            "   [2.37617371e+02 1.71954071e+02 1.55714502e+01]\n",
            "   [2.37285721e+02 1.67285721e+02 1.72857285e+01]\n",
            "   [2.36637680e+02 1.66209106e+02 1.84233913e+01]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[2.50994858e+02 2.14994858e+02 1.02137672e+02]\n",
            "   [2.51499954e+02 2.16428528e+02 1.00571381e+02]\n",
            "   [2.49168365e+02 2.14474487e+02 9.38316269e+01]\n",
            "   ...\n",
            "   [2.48453934e+02 2.08453934e+02 9.44539413e+01]\n",
            "   [2.53341843e+02 2.13397980e+02 1.02612297e+02]\n",
            "   [2.50157867e+02 2.08877228e+02 1.04719170e+02]]\n",
            "\n",
            "  [[2.51948975e+02 2.16020416e+02 9.29948273e+01]\n",
            "   [2.51346985e+02 2.17418427e+02 9.11938782e+01]\n",
            "   [2.50096954e+02 2.17096954e+02 8.78111877e+01]\n",
            "   ...\n",
            "   [2.49826569e+02 2.13826569e+02 9.36122360e+01]\n",
            "   [2.51852020e+02 2.14918350e+02 9.96529999e+01]\n",
            "   [2.50433517e+02 2.12433517e+02 1.03953957e+02]]\n",
            "\n",
            "  [[2.53714355e+02 2.20224609e+02 8.55102539e+01]\n",
            "   [2.51933472e+02 2.17933472e+02 8.32191162e+01]\n",
            "   [2.49668304e+02 2.18025482e+02 7.99539413e+01]\n",
            "   ...\n",
            "   [2.52291000e+02 2.17291000e+02 8.85766525e+01]\n",
            "   [2.50785889e+02 2.14785889e+02 9.25971146e+01]\n",
            "   [2.48816376e+02 2.11816376e+02 9.65867767e+01]]]\n",
            "\n",
            "\n",
            " [[[5.58673477e+00 5.58673477e+00 3.58673477e+00]\n",
            "   [5.35714293e+00 5.35714293e+00 3.35714293e+00]\n",
            "   [6.49489784e+00 6.49489784e+00 4.49489784e+00]\n",
            "   ...\n",
            "   [1.51147903e+02 1.28086685e+02 1.20142876e+02]\n",
            "   [1.49959183e+02 1.29765335e+02 1.24362259e+02]\n",
            "   [1.49510147e+02 1.31096909e+02 1.27096909e+02]]\n",
            "\n",
            "  [[5.00000000e+00 5.00000000e+00 3.00000000e+00]\n",
            "   [4.13775444e+00 4.13775444e+00 2.13775468e+00]\n",
            "   [5.07142830e+00 5.07142830e+00 3.07142830e+00]\n",
            "   ...\n",
            "   [1.55443924e+02 1.30903168e+02 1.20816452e+02]\n",
            "   [1.57428528e+02 1.34846924e+02 1.28709152e+02]\n",
            "   [1.48311096e+02 1.28117279e+02 1.22714180e+02]]\n",
            "\n",
            "  [[6.21428585e+00 6.21428585e+00 4.21428585e+00]\n",
            "   [5.27040815e+00 5.27040815e+00 3.27040815e+00]\n",
            "   [5.00000000e+00 5.00000000e+00 3.00000000e+00]\n",
            "   ...\n",
            "   [1.53831696e+02 1.26617432e+02 1.15403168e+02]\n",
            "   [1.55698959e+02 1.31056137e+02 1.23341835e+02]\n",
            "   [1.54010147e+02 1.31852066e+02 1.25142853e+02]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[1.12285713e+02 4.04999771e+01 1.48571863e+01]\n",
            "   [1.07285690e+02 3.62295685e+01 8.39793968e+00]\n",
            "   [1.20571426e+02 4.95714264e+01 2.08061562e+01]\n",
            "   ...\n",
            "   [2.32188553e+02 1.91402817e+02 1.29402817e+02]\n",
            "   [2.24913193e+02 1.84127457e+02 1.22127457e+02]\n",
            "   [2.24280472e+02 1.83494736e+02 1.21494728e+02]]\n",
            "\n",
            "  [[1.14571419e+02 4.25714149e+01 1.75714149e+01]\n",
            "   [1.07209183e+02 3.52091827e+01 1.02091818e+01]\n",
            "   [1.17413231e+02 4.63571014e+01 1.85255013e+01]\n",
            "   ...\n",
            "   [2.25413162e+02 1.83413162e+02 1.23413162e+02]\n",
            "   [2.25000000e+02 1.83000000e+02 1.23000000e+02]\n",
            "   [2.26382690e+02 1.84382690e+02 1.24382698e+02]]\n",
            "\n",
            "  [[1.12744942e+02 4.27449417e+01 1.77449436e+01]\n",
            "   [1.09117447e+02 3.91174469e+01 1.31174488e+01]\n",
            "   [1.17137848e+02 4.73521385e+01 2.07092800e+01]\n",
            "   ...\n",
            "   [2.19714172e+02 1.77714172e+02 1.19714172e+02]\n",
            "   [2.22974487e+02 1.80974487e+02 1.22974480e+02]\n",
            "   [2.25071533e+02 1.83071533e+02 1.25071533e+02]]]\n",
            "\n",
            "\n",
            " [[[2.39897964e+02 2.36255112e+02 2.23826538e+02]\n",
            "   [2.44790817e+02 2.37816330e+02 2.28959183e+02]\n",
            "   [2.44025513e+02 2.32811218e+02 2.31668365e+02]\n",
            "   ...\n",
            "   [2.26581680e+02 2.10581680e+02 2.22010208e+02]\n",
            "   [2.24994888e+02 2.08994888e+02 2.18994888e+02]\n",
            "   [2.20137650e+02 2.05137650e+02 2.12137650e+02]]\n",
            "\n",
            "  [[2.40209183e+02 2.36280624e+02 2.36852051e+02]\n",
            "   [2.39551025e+02 2.34352036e+02 2.31336731e+02]\n",
            "   [2.43755096e+02 2.34285706e+02 2.35770416e+02]\n",
            "   ...\n",
            "   [2.23989990e+02 2.07989990e+02 2.19418518e+02]\n",
            "   [2.18561295e+02 2.02561295e+02 2.12561295e+02]\n",
            "   [2.26168030e+02 2.11168030e+02 2.18168030e+02]]\n",
            "\n",
            "  [[2.45846939e+02 2.40489792e+02 2.50275513e+02]\n",
            "   [2.40459183e+02 2.34887741e+02 2.45943878e+02]\n",
            "   [2.35525513e+02 2.29051025e+02 2.44357147e+02]\n",
            "   ...\n",
            "   [2.23964432e+02 2.07964432e+02 2.19729706e+02]\n",
            "   [2.18071472e+02 2.02071472e+02 2.13015335e+02]\n",
            "   [2.26372330e+02 2.10372330e+02 2.20372330e+02]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[2.27387726e+02 2.21387726e+02 2.23387726e+02]\n",
            "   [2.26724457e+02 2.20724457e+02 2.22724457e+02]\n",
            "   [2.30933685e+02 2.24290833e+02 2.26505112e+02]\n",
            "   ...\n",
            "   [2.33954086e+02 2.30596878e+02 2.28954086e+02]\n",
            "   [2.32127518e+02 2.28770309e+02 2.27127518e+02]\n",
            "   [2.34214447e+02 2.30857239e+02 2.29214447e+02]]\n",
            "\n",
            "  [[2.29862259e+02 2.23862259e+02 2.25862259e+02]\n",
            "   [2.28918396e+02 2.22918396e+02 2.24918396e+02]\n",
            "   [2.30612228e+02 2.23969376e+02 2.26183655e+02]\n",
            "   ...\n",
            "   [2.34612305e+02 2.28612305e+02 2.30612305e+02]\n",
            "   [2.34142883e+02 2.28142883e+02 2.30142883e+02]\n",
            "   [2.35045929e+02 2.29045929e+02 2.31045929e+02]]\n",
            "\n",
            "  [[2.29280563e+02 2.23280563e+02 2.25280563e+02]\n",
            "   [2.30295868e+02 2.24295868e+02 2.26295868e+02]\n",
            "   [2.31367355e+02 2.24724503e+02 2.26938797e+02]\n",
            "   ...\n",
            "   [2.39005173e+02 2.30005173e+02 2.35005173e+02]\n",
            "   [2.36785706e+02 2.27785706e+02 2.32785706e+02]\n",
            "   [2.38642822e+02 2.29642822e+02 2.34642822e+02]]]], shape=(32, 224, 224, 3), dtype=float32) tf.Tensor(\n",
            "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]], shape=(32, 10), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resNet_model = tf.keras.applications.ResNet50(include_top=False,weights='imagenet',input_shape=(224,224,3))\n"
      ],
      "metadata": {
        "id": "eEtPK_j5Biq0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "521a5307-947d-4ea5-b294-f4cf29133f6a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resNet_weights = resNet_model.get_weights()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kTGMmF05G-oc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resNet_model.compile(loss='categorical_crossentropy',\n",
        "                     optimizer='adam',\n",
        "                     metrics=['accuracy'])\n",
        "# resNet_model.fit(train_data_10_percent,epochs=3)"
      ],
      "metadata": {
        "id": "7BbVk_MNH3zT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the model using functional API.\n",
        "what is functional API actually"
      ],
      "metadata": {
        "id": "B4WTIhMhhSou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getMe():\n",
        "  def hello(name):\n",
        "    return f\"hello {name} functional API\"\n",
        "  return hello\n",
        "\n",
        "result = getMe()(\"Keras\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xd3M_d0gjLd",
        "outputId": "ea6b1076-8828-4956-db04-c7f129dbdc8f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello Keras functional API\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modle 0: Building a transfer learning model using the Keras Functional API\n",
        "\n"
      ],
      "metadata": {
        "id": "N-TOzdh-nuPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create the base model with tf.keras.applications.resNet50\n",
        "base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n",
        "\n",
        "# 2. Freeze the base model (so the underling pre-trained patterns aren't updated durring training)\n",
        "base_model.trainable = False\n",
        "\n",
        "# 3. Create inputs into our model\n",
        "inputs = tf.keras.layers.Input(shape=(224,224,3), name=\"input_layer\")\n",
        "\n",
        "# 4. If you using a model like ResNet50V2 you will need to normalize inputs\n",
        "# x = tf.keras.layers.experimental.preprocessing.Rescaling(1/255.)(inputs)\n",
        "\n",
        "# 5. Pass the inputs to the base_model\n",
        "x = base_model(inputs)\n",
        "print(f\"Shape after passing inputs through base model: {x.shape}\")\n",
        "\n",
        "# 6. Average pool the outputs of the base model (aggregate all the the most important information reduce number of computations)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\n",
        "print(f\"Shape after global average 2D: {x.shape}\")\n",
        "\n",
        "# 7. Create the output activation layer\n",
        "outputs = tf.keras.layers.Dense(10,activation=\"softmax\",name=\"output_layer\")(x)\n",
        "\n",
        "# 8. Combine the inputs with the outputs into a model\n",
        "model_0 = tf.keras.Model(inputs,outputs)\n",
        "\n",
        "# 9. compile the model\n",
        "model_0.compile(loss=\"categorical_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "# 10. Fit the model and save its history\n",
        "model_0_history = model_0.fit(train_data_10_percent,\n",
        "                              epochs=5,\n",
        "                              steps_per_epoch=len(train_data_10_percent),\n",
        "                              validation_data= test_data,\n",
        "                              validation_steps= len(test_data),\n",
        "                              callbacks=[create_tensorboard_callback(\"TensorFlow_Hub\",\"Fine_tuned_EfficientNet\")]\n",
        "                              )\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EE46ggRioEpu",
        "outputId": "bd262d2c-8976-436a-b6ff-ab648311fa87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Shape after passing inputs through base model: (None, 7, 7, 1280)\n",
            "Shape after global average 2D: (None, 1280)\n",
            "Saving TensorBoard log files to: TensorFlow_Hub/Fine_tuned_EfficientNet/20251018-145100\n",
            "Epoch 1/5\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.2262 - loss: 2.1878"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curves(model_0_history)"
      ],
      "metadata": {
        "id": "UiXEpHFzx7vD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the tuned model\n",
        "model_0.evaluate(test_data)"
      ],
      "metadata": {
        "id": "ygr3yPdszZyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer_number, layer in enumerate(base_model.layers):\n",
        "  print(layer_number,layer.name)"
      ],
      "metadata": {
        "id": "IpNcBRWh5JjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.summary()"
      ],
      "metadata": {
        "id": "73At0JB70L7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.keras.utils.plot_model(base_model)"
      ],
      "metadata": {
        "id": "o-1tj_xMzm3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.summary()"
      ],
      "metadata": {
        "id": "M-0sISVQ1dr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting a feature vector from a trained model"
      ],
      "metadata": {
        "id": "pXRHVwrp6llo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define the input shape\n",
        "# input_shape = (1,4,4,3)\n",
        "# # Create a random tensor\n",
        "# tf.random.set_seed(42)\n",
        "# input_tensor = tf.random.normal(input_shape)\n",
        "# # pass the random tensor through a global average pooling 2D layer\n",
        "# global_average_pooled_tensor = tf.keras.layers.GlobalAveragePooling2D(input_tensor)\n",
        "# global_average_pooled_tensor"
      ],
      "metadata": {
        "id": "wkNcBEgw9Tjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running a series of transfer learning experiments\n",
        "we've seen the incredible results transfer learning can get with only 10% of the training data, but how does it go with 1% of the training data.. how about we set up a bunch of experiments to find out:\n",
        "1. `model_1`  - use feature extracion transfer learning with 1% of the training data with data augmentation\n",
        "2. `model_2` - use feature extracctio transfer learning with 10% of the training data with data augmentation\n",
        "3. `model_3` - use fine-tuning transfer learning on 10% of the training data with data augmentation\n",
        "4. `model_4` - use fine-turning transfer learning on 100% of the training data with data augmentation\n",
        "**Note:** throught all experiments the same test dataset will be used to evaluate our model.. this ensures consisency across evaluation metrics.\n"
      ],
      "metadata": {
        "id": "1pOEU7yi60j3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CX0pwn9xzn3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_1_percent.zip"
      ],
      "metadata": {
        "id": "fqK_9qva8pJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unzip_data(\"10_food_classes_1_percent.zip\")"
      ],
      "metadata": {
        "id": "y6Twz4Qe8tBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "walk_through_dir(\"10_food_classes_1_percent\")"
      ],
      "metadata": {
        "id": "OoBhXA8d82ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir_1_percent = \"10_food_classes_1_percent/train\"\n",
        "test_dir = \"10_food_classes_1_percent/test\"\n"
      ],
      "metadata": {
        "id": "_6cuAVmy88Nj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up data loader\n",
        "IMG_SIZE = (224,224)\n",
        "BATCH_SIZE = 32\n",
        "train_data_1_percent = tf.keras.preprocessing.image_dataset_from_directory(directory=train_dir_1_percent,\n",
        "                                                             image_size=IMG_SIZE,\n",
        "                                                             label_mode=\"categorical\",\n",
        "                                                             batch_size=BATCH_SIZE,\n",
        "                                                             )\n",
        "test_data = tf.keras.preprocessing.image_dataset_from_directory(directory=test_dir,\n",
        "                                                  image_size=IMG_SIZE,\n",
        "                                                  batch_size=BATCH_SIZE,\n",
        "                                                  label_mode=\"categorical\",\n",
        "                                                  )"
      ],
      "metadata": {
        "id": "Jgb5wunk9Tke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import preprocessing\n"
      ],
      "metadata": {
        "id": "vMCqYoLZC4F5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = tf.keras.Sequential([\n",
        "    # tf.keras.layers.Rescaling(1/255.), # EfficientNet has rescaling built-in\n",
        "    # tf.keras.layers.Input(shape=(224,224,3)),\n",
        "    tf.keras.layers.RandomFlip('horizontal'),\n",
        "    tf.keras.layers.RandomRotation(0.2),\n",
        "    tf.keras.layers.RandomHeight(0.2),\n",
        "    tf.keras.layers.RandomWidth(0.2),\n",
        "    # tf.keras.layers.RandomErasing(factor=1,scale=(0.02,0.03)),\n",
        "    tf.keras.layers.RandomZoom(height_factor=(0.1,0.15)),\n",
        "    ],name=\"data_augmentation\")"
      ],
      "metadata": {
        "id": "Gzk0nCME-5Jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K94r7kSqXwmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# view random image and compare with original image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import os\n",
        "import random\n",
        "target_class = random.choice(train_data_1_percent.class_names)\n",
        "target_dir = \"10_food_classes_1_percent/train/\" + target_class\n",
        "random_image = random.choice(os.listdir(target_dir))\n",
        "random_image_path = os.path.join(target_dir,random_image)\n",
        "img = mpimg.imread(random_image_path)\n",
        "plt.imshow(img)\n",
        "plt.title(f\"original random image from class: {target_class}\")\n",
        "plt.axis(False)\n",
        "plt.figure();\n",
        "augmented_imag = data_augmentation(img)\n",
        "print(augmented_imag.shape)\n",
        "plt.imshow(augmented_imag/255.)\n",
        "plt.title(f\"augmented random image from class: {target_class}\")\n",
        "plt.axis(False)\n"
      ],
      "metadata": {
        "id": "0uZMztThHZS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1: feature extraction transfer learning with 1% of the data with data augmentation"
      ],
      "metadata": {
        "id": "I24ubEbfFASR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#data augmentation with functional layers\n",
        "# Recommended way for TF ≥ 2.17\n",
        "inputs = tf.keras.Input(shape=(224, 224, 3), name=\"input_layer\")\n",
        "\n",
        "# Apply augmentation functionally (not in a Sequential)\n",
        "x = tf.keras.layers.RandomFlip(\"horizontal\")(inputs)\n",
        "x = tf.keras.layers.RandomRotation(0.2)(x)\n",
        "x = tf.keras.layers.RandomHeight(0.2)(x)\n",
        "x = tf.keras.layers.RandomWidth(0.2)(x)\n",
        "x = tf.keras.layers.RandomZoom(0.1, 0.15)(x)\n",
        "# Setup input shape and base_model\n",
        "input_shape = (224,224,3)\n",
        "base_model = tf.keras.applications.EfficientNetB0(include_top=False,input_shape=(224,224,3))\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create the input layer\n",
        "# inputs = layers.Input(shape=input_shape,name=\"input_layer\")\n",
        "\n",
        "# add in data augmentation Sequential model as a layer\n",
        "# x = data_augmentation(inputs)\n",
        "\n",
        "# Give base_model the inputs( after augmentation) and don't train it\n",
        "x = base_model(x,training=False)\n",
        "\n",
        "# pool output features of the base model\n",
        "x = layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\n",
        "\n",
        "# put a dense layer on as the output\n",
        "outputs = layers.Dense(10,activation=\"softmax\",name=\"output_layer\")(x)\n",
        "\n",
        "# Make a model using the inputs and outputs\n",
        "model_1 = keras.Model(inputs=inputs,outputs=outputs)\n",
        "\n",
        "# compile the model\n",
        "model_1.compile(loss=\"categorical_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "# fit the model\n",
        "model_1_history = model_1.fit(train_data_1_percent,\n",
        "                              epochs=5,\n",
        "                              steps_per_epoch=len(train_data_1_percent),\n",
        "                              validation_data=test_data,\n",
        "                              validation_steps=(int(0.25*len(test_data))),\n",
        "                              callbacks=[create_tensorboard_callback(\"TensorFlow_Hub\",\"1_percent_data_exprt\")]\n",
        "                              )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Eb8zcI7uPBFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curves(model_1_history)"
      ],
      "metadata": {
        "id": "WeVkcSAQZ3wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.summary()"
      ],
      "metadata": {
        "id": "rDNvxZ87XDzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate with test dataset\n",
        "model_1.evaluate(test_data)"
      ],
      "metadata": {
        "id": "N4GAWoV3m00K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 2: feature extraction transfer learning model with 10% of data and data augmentation"
      ],
      "metadata": {
        "id": "c9-3MGFJnHYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get 10% of data\n",
        "train_dir_10_percent = \"/content/10_food_classes_10_percent/train\"\n",
        "test_dir = \"/content/10_food_classes_10_percent/test\"\n"
      ],
      "metadata": {
        "id": "C9oyEbqwn_UL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set data inputs\n",
        "IMG_SIZE = (224,224)\n",
        "train_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(train_dir_10_percent,\n",
        "                                                                            label_mode=\"categorical\",\n",
        "                                                                            image_size=IMG_SIZE\n",
        "                                                                            )\n",
        "test_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir,\n",
        "                                                                label_mode=\"categorical\",\n",
        "                                                                image_size=IMG_SIZE)"
      ],
      "metadata": {
        "id": "x6NQUZdCoeqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model 2 with data augmentation built in\n",
        "from tensorflow.keras import layers\n",
        "inputs = layers.Input(shape=(224,224,3),name=\"input_layer\")\n",
        "x = layers.RandomFlip(\"horizontal\")(inputs)\n",
        "x = layers.RandomHeight(0.2)(x)\n",
        "x = layers.RandomWidth(0.2)(x)\n",
        "x = layers.RandomZoom(0.2)(x)\n",
        "x = layers.RandomRotation(0.2)\n",
        "\n",
        "# setup the input shape to our model\n",
        "input_shape = (224,224,3)\n",
        "# create a frozen base model\n",
        "base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n",
        "base_model.trainable = False\n",
        "x = base_model(x,trainabl=False)\n",
        "x = layers.GlobalAveragePooling2D(name=\"Global_average_pooling_layer\")(x)\n",
        "outputs =  layers.Dense(10,activation=\"softmax\",name=\"output_layer\")(x)\n",
        "model_2 = tf.keras.Model(inputs,outputs)\n",
        "\n",
        "# compile the model\n",
        "model_2.compile(loss=\"categorical_crossentropy\",\n",
        "                optimizer=\"adam\",\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "# Fit the model\n",
        "model_2_history = model_2.fit(train_data_10_percent,\n",
        "                              epochs=5,\n",
        "                              steps_per_epoch=len(train_data_10_percent),\n",
        "                              validation_data=test_data,\n",
        "                              validation_steps=int(0.25*len(test_data)))\n",
        "\n"
      ],
      "metadata": {
        "id": "nShhAAXPpbuY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}