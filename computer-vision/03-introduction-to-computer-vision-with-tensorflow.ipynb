{"cells":[{"cell_type":"markdown","metadata":{"id":"UuSK0kZcxFru"},"source":["# Introduction to Convolutional Neural Network and computer Vision with Tensorflow\n","Computer vision is the practice of writing algorithms which can discover pattern in visual data. Such as the camera of self driving car recognizing the car in front."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"s-gYVrSpMA1c"},"outputs":[],"source":["# !pip install tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"GKihsVvLxmLa"},"outputs":[],"source":["import zipfile\n","\n","! wget https://storage.googleapis.com/ztm_tf_course/food_vision/pizza_steak.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"iZgHMvqt3QI4"},"outputs":[],"source":["# Unzip the file\n","zip_ref = zipfile.ZipFile(\"pizza_steak.zip\")\n","zip_ref.extractall()\n","zip_ref.close()"]},{"cell_type":"markdown","metadata":{"id":"OqEKjv8e5cH5"},"source":["## Inspect the data ( become one with it)\n","A very crucial step at the beginning of any machine learning project is becoming one with the data.\n","And for a computer vision project .... this usually mean visualize many sample of your data.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"fY5ThizQ5Syz"},"outputs":[],"source":["!ls pizza_steak"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"AfsyKyJ957e9"},"outputs":[],"source":["!ls pizza_steak/train/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"TTouJ2fr6A2l"},"outputs":[],"source":["!ls pizza_steak/train/pizza"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Zp93_S7g6T85"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"fK0KUzGk6hAd"},"outputs":[],"source":["import os\n","import tensorflow as tf\n","\n","# walk through pizza_steak directory and list number of files\n","for dirpath, dirnames,filesnames in os.walk(\"pizza_steak\"):\n","  print(f\"There are {len(dirnames)} directories and {len(filesnames)} images in {dirpath} \")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"8XTJ4jIX7JBv"},"outputs":[],"source":["# Another way to find out how many images are in a file\n","num_steak_images_train = len(os.listdir(\"pizza_steak/train/steak\"))\n","num_pizza_images_train =len(os.listdir(\"pizza_steak/train/pizza\"))\n","(num_steak_images_train,num_pizza_images_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-6ZEI1DtCU9_"},"outputs":[],"source":["# Get the classnames programmatically\n","import pathlib\n","import numpy as np\n","data_dir = pathlib.Path(\"pizza_steak/train\")\n","class_names = np.array(sorted([item.name for item in data_dir.glob(\"*\")]))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"XNV-6Dj9H2Cz"},"outputs":[],"source":["# Let's visualize our images\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import random\n","def view_random_image(target_dir,target_class):\n","  # Setup the target directory (we'll view images from here)\n","  target_folder = target_dir + \"/\" + target_class\n","  # Get a random image path\n","  random_image = random.sample(os.listdir(target_folder),1)\n","  # Read in the image and plot it using matplotlib\n","  img = mpimg.imread(target_folder + \"/\" + random_image[0])\n","  plt.imshow(img)\n","  plt.title(target_class)\n","  plt.axis(\"off\");\n","  print(f\"Image shape: {img.shape}\") # show the shape of the image\n","  return img\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"lElfpiYZKhyB"},"outputs":[],"source":["# View a random image from the training dataset\n","img = view_random_image(target_dir=\"pizza_steak/train\",target_class=\"pizza\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"XmK9JUV3NOHV"},"outputs":[],"source":["tensor_image = tf.constant(img/255.0)\n","tf.shape(tensor_image),tensor_image"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"JNrYJciFas8s"},"outputs":[],"source":["\"\"\"\n","Load our images\n","preprocess our images\n","Build a CNN to find pattterns in our images\n","Compile our CNN\n","fit the CNN to our training data\n","\n","\n","\"\"\"\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","# Set the seed\n","tf.random.set_seed(42)\n","# Preprocess data (get all of the pixel values between 0 & 1,also called normalize/scaling)\n","train_datagen = ImageDataGenerator(rescale=1./255)\n","valid_datagen = ImageDataGenerator(rescale=1./255)\n","# Setup paths to our data directories\n","train_dir = \"/content/pizza_steak/train\"\n","test_dir = \"/content/pizza_steak/test\"\n","# Import data from directories and turn it into batches\n","train_data = train_datagen.flow_from_directory(directory=train_dir,\n","                                               batch_size=32,\n","                                               target_size=(224,224),\n","                                               class_mode=\"binary\",\n","                                               seed=42)\n","valid_data = valid_datagen.flow_from_directory(directory=test_dir,\n","                                               batch_size=32,\n","                                               target_size=(224,224),\n","                                               class_mode=\"binary\",\n","                                               seed=42)\n","# Build a CNN model\n","model_1 = tf.keras.models.Sequential([\n","    tf.keras.layers.Conv2D(filters=10,\n","                           kernel_size=3,\n","                           activation=\"relu\",\n","                           input_shape=(224,224,3)),\n","\n","    tf.keras.layers.Conv2D(10,3,activation=\"relu\"),\n","\n","    tf.keras.layers.MaxPool2D(pool_size=2,\n","                              padding=\"valid\"),\n","    tf.keras.layers.Conv2D(10,3,activation=\"relu\"),\n","    tf.keras.layers.Conv2D(10,3,activation=\"relu\"),\n","    tf.keras.layers.MaxPool2D(2),\n","    tf.keras.layers.Flatten(),\n","    # tf.keras.layers.Dense(280,activation=\"relu\"),\n","    # tf.keras.layers.Dense(140,activation=\"relu\"),\n","    # tf.keras.layers.Dense(70,activation=\"relu\"),\n","    # tf.keras.layers.Dense(30,activation=\"relu\"),\n","    # tf.keras.layers.Dense(5,activation=\"relu\"),\n","    tf.keras.layers.Dense(1,activation=\"sigmoid\")\n","    ])\n","# Compile our CNN\n","model_1.compile(loss=\"binary_crossentropy\",\n","                optimizer=tf.keras.optimizers.SGD(),\n","                metrics=[\"accuracy\"])\n","# Fit the model\n","history_model_1 = model_1.fit(train_data,\n","                              epochs=5,\n","                              steps_per_epoch=len(train_data),\n","                              validation_data= valid_data,\n","                              validation_steps=len(valid_data))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"tpwBwyuynG1v"},"outputs":[],"source":["model_1.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ZyFQDmimChdU"},"outputs":[],"source":["from keras.utils import plot_model\n","plot_model(model_1,show_shapes=True,show_layer_names=True,expand_nested=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"9tnepDVvDj42"},"outputs":[],"source":["plt.plot(range(len(history_model_1.history[\"accuracy\"])),history_model_1.history[\"accuracy\"],label=\"accuracy\", color=\"green\")\n","plt.plot(range(len(history_model_1.history[\"loss\"])),history_model_1.history[\"loss\"],label=\"loss\",color=\"red\")\n","plt.title(\"loss and accuracy of food classification CNN model over 25 epochs\")\n","plt.xlabel(\"epochs\")\n","plt.ylabel(\"loss and accuracy\")\n","plt.legend();"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"mSX1iLv4D2Ym"},"outputs":[],"source":["# Set random seed\n","tf.random.set_seed(42)\n","\n","# Create a model to replicate\n","model_2 = tf.keras.Sequential([\n","    tf.keras.layers.Flatten(input_shape=(224,224,3)),\n","    tf.keras.layers.Dense(4,activation=\"relu\"),\n","    tf.keras.layers.Dense(4,activation=\"relu\"),\n","    tf.keras.layers.Dense(1,activation=\"sigmoid\")\n","\n","])\n","\n","# Compile the model\n","model_2.compile(loss=\"binary_crossentropy\",\n","                optimizer=tf.keras.optimizers.Adam(),\n","                metrics=[\"accuracy\"]\n","                )\n","# Fit the model\n","history_2 = model_2.fit(train_data,\n","                        epochs=5,\n","                        steps_per_epoch=len(train_data),\n","                        validation_data=valid_data,\n","                        validation_steps=len(valid_data)\n","                        )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"l58DlyjAW9pc"},"outputs":[],"source":["# Set random seed\n","tf.random.set_seed(42)\n","# create the model\n","model_3 = tf.keras.Sequential([\n","    tf.keras.layers.Flatten(input_shape=(224,224,3)),\n","    tf.keras.layers.Dense(100,activation=\"relu\"),\n","    tf.keras.layers.Dense(100,activation=\"relu\"),\n","    tf.keras.layers.Dense(100,activation=\"relu\"),\n","    tf.keras.layers.Dense(1,activation=\"sigmoid\")\n","])\n","# Compile the model\n","model_3.compile(loss=\"binary_crossentropy\",\n","                optimizer=\"adam\",\n","                metrics=[\"accuracy\"])\n","# Fit the model\n","history_3 = model_3.fit(train_data,\n","                        epochs=5,\n","                        steps_per_epoch=len(train_data),\n","                        validation_data=valid_data,\n","                        validation_steps=len(valid_data))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"hdahnbp7Z9tR"},"outputs":[],"source":["model_2.summary(),model_3.summary()"]},{"cell_type":"markdown","metadata":{"id":"a3YStBWPiGW7"},"source":["## Binary classification: Let's break it down\n","1. Become on with the data(visualize,visualize,visualize)\n","2. Preprocess the data(prepared it for our model, the main step her was scaling/normalizing)\n","3. Create a model(start with abaseline)\n","4. Fit the model\n","5. Evaluate the model\n","6. adjust different parameters and improve the model(try to beat our baseline)\n","7. Repeat until satisfied (experiment,experiment,experiment)"]},{"cell_type":"markdown","metadata":{"id":"i0tAX4vOjARD"},"source":["### Become one with data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"aiddDMEHcVGe"},"outputs":[],"source":["# Visualize data\n","plt.figure()\n","plt.subplot(1,2,1)\n","steak_img = view_random_image(\"pizza_steak/train/\",\"steak\")\n","plt.subplot(1,2,2)\n","pizza_img = view_random_image(\"pizza_steak/train/\",\"pizza\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"k2r03d4amHVd"},"source":["### Preproccess the data( prepare it for a model)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"iya-hnH1l3cH"},"outputs":[],"source":["# Define directory dataset paths\n","train_dir = \"pizza_steak/train\"\n","test_dir = \"pizza_steak/test\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Qj9bZ4jrmqIG"},"outputs":[],"source":["# tern our data into Batchs\n","!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ke5Pxh3-nrja"},"outputs":[],"source":["# Create train and test data generators and rescale the data\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","train_datagen = ImageDataGenerator(rescale=1./255)\n","test_datagen = ImageDataGenerator(rescale=1./255)\n","# Load in our image data from directories and trun them into batches\n","train_data = train_datagen.flow_from_directory(directory=train_dir,\n","                                               target_size=(224,224),\n","                                               class_mode=\"binary\",\n","                                               batch_size=32)\n","test_data = test_datagen.flow_from_directory(directory=test_dir,\n","                                               target_size=(224,224),\n","                                               class_mode=\"binary\",\n","                                               batch_size=32)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"S-2v2dzaxdlR"},"outputs":[],"source":["# Get a sample of a train data batch\n","print(len(train_data))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"6uEUaHxD80Vk"},"outputs":[],"source":["# Make the creating of our model a little easier\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D,Activation\n","from tensorflow.keras import Sequential\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ZjkxdGs78-zN"},"outputs":[],"source":["# Create the model\n","model_4 = Sequential([\n","    Conv2D(filters=10,\n","           kernel_size=3,\n","           strides=1,\n","           padding=\"valid\",\n","           activation=\"relu\",\n","           input_shape=(224,224,3)), # input layer\n","\n","    Conv2D(10,3, activation=\"relu\"),\n","    Conv2D(10,3, activation=\"relu\"),\n","    Flatten(),\n","    Dense(1,activation=\"sigmoid\") # output layer\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"L7bls_rlfHgN"},"outputs":[],"source":["# compile the model\n","model_4.compile(loss=\"binary_crossentropy\",\n","                optimizer=Adam(),\n","                metrics=[\"accuracy\"])\n","model_4.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"psiJXOFmfyDx"},"outputs":[],"source":["len(train_data),len(test_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"1Qc399Q-fcmN"},"outputs":[],"source":["# fit the model\n","history_model_4 = model_4.fit(train_data,\n","            epochs=5,\n","            steps_per_epoch=len(train_data),\n","            validation_data = test_data,\n","            validation_steps=len(test_data)\n","            )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"VLkWunDchJT9"},"outputs":[],"source":["model_1.evaluate(test_data),model_4.evaluate(test_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"OCXkX5onGEsj"},"outputs":[],"source":["# Let's plot the training curves\n","import pandas as pd\n","pd.DataFrame(history_model_4.history).plot(figsize=(7,5))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"INlt4BzHGuNY"},"outputs":[],"source":["# plot the validation and training curves separatedly\n","def plot_loss_curves(history):\n","  \"\"\"\n","  Returns separate loss curves for training and validation metrics\n","  \"\"\"\n","  loss = history.history[\"loss\"]\n","  val_loss = history.history[\"val_loss\"]\n","  accuracy = history.history[\"accuracy\"]\n","  val_accuracy = history.history[\"val_accuracy\"]\n","  epochs = range(len(history.history[\"loss\"]))\n","  # Plot loss\n","  plt.figure();\n","  plt.plot(epochs,loss,label=\"training_loss\",color=\"red\")\n","  plt.plot(epochs,val_loss,label=\"validation_loss\")\n","  plt.title(\"loss\")\n","  plt.xlabel(\"epoch\")\n","  plt.legend()\n","\n","  #plot accuracy\n","  plt.figure();\n","  plt.plot(epochs,accuracy,label=\"training_accuracy\",color=\"green\")\n","  plt.plot(epochs,val_accuracy,label=\"validation_accuracy\")\n","  plt.title(\"validation\")\n","  plt.xlabel(\"epochs\")\n","  plt.legend()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"79DPcuqTXvo1"},"outputs":[],"source":["plot_loss_curves(history=history_model_4)"]},{"cell_type":"markdown","metadata":{"id":"-CPvqW_jYkDV"},"source":[" #### Adjust te model parameters\n"," Fitting a machine learning model comes in 3 steps:\n","\n"," 0. Create a baseline\n"," 1. Beat the beseline by overfitting a larger model\n"," 2. Reduce overfitting\n","\n"," Ways to induce overfitting:\n"," * Increase the number of conv layers\n"," * Increase the number of conv filters\n"," * add another dense layer to the ouput of our flattened layer\n","\n"," Reduce overfitting:\n"," * add data augmentation\n"," * add regularization layers(such as MaxPool23)\n"," * add more data...\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"gLXyItxmeROz"},"outputs":[],"source":["# Create the model (this is going to be our new baseline)\n","model_5 = Sequential([\n","    Conv2D(10,3,activation=\"relu\",input_shape=(224,224,3)),\n","    MaxPool2D(pool_size=2),\n","    Conv2D(10,3,activation=\"relu\"),\n","    MaxPool2D(),\n","    Conv2D(10,3,activation=\"relu\"),\n","    MaxPool2D(),\n","    Flatten(),\n","    Dense(1,activation=\"sigmoid\")\n","\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"IlyPiip3hwwI"},"outputs":[],"source":["# Compile the model\n","model_5.compile(loss=\"binary_crossentropy\",\n","                optimizer=Adam(),\n","                metrics=[\"accuracy\"])\n","# Fit the model\n","history_model_5 = model_5.fit(train_data,\n","                              epochs=5,\n","                              steps_per_epoch=len(train_data),\n","                              validation_data=test_data,\n","                              validation_steps=len(test_data)\n","                              )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"bzMkZRfmlqAf"},"outputs":[],"source":["# Get a summary our model with max pooling\n","model_5.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"XARmhAhymQ-5"},"outputs":[],"source":["model_4.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"XjBqphtamZw7"},"outputs":[],"source":["# plot training curves of model_5\n","plot_loss_curves(history_model_5)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"2zhzKW6KoevQ"},"outputs":[],"source":["# compare model_4 and model_5 so as to know the power of maxpool\n","comparision_data = dict()\n","comparision_data[\"accuracy\"] = history_model_5.history[\"accuracy\"]\n","comparision_data[\"val_accuracy\"] = history_model_4.history[\"accuracy\"]\n","comparision_data[\"loss\"] = history_model_5.history[\"loss\"]\n","comparision_data[\"val_loss\"] = history_model_4.history[\"loss\"]\n","\n","# plot comparition curve\n","# plot_loss_curves(comparision_data)\n"]},{"cell_type":"markdown","metadata":{"id":"VVUvf6COsQJO"},"source":["### Opening our bag of tricks and finding data augmentation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"TvdEMke2sZMK"},"outputs":[],"source":["# Create ImageDataGenerator training instance with data augmentation\n","train_datagen_augmented = ImageDataGenerator(rescale=1/255.,\n","                                             rotation_range=0.2,\n","                                             shear_range=0.2,\n","                                             zoom_range=0.2,\n","                                             width_shift_range=0.2,\n","                                             height_shift_range=0.2,\n","                                             horizontal_flip=True\n","                                             )\n","# Create ImageDataGenerator without data augmentation\n","train_datagen =  ImageDataGenerator(rescale=1/255.)\n","\n","# Create ImageDataGenerator without data augmentation for the test dataset\n","test_datagen = ImageDataGenerator(rescale=1/255.)"]},{"cell_type":"markdown","metadata":{"id":"93kH9wW_sVaZ"},"source":["> **Question:** what is data augmentation?\n","Data augmentation is the process of altering our training data leading it to have more diversity and in turn allowing our models to learn more genralizable ( hopefully) patterns.\n","Altering might mean adjusting the rotation of an image, flipping it, cropping it or something similar."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"RgtvYt5zwWKm"},"outputs":[],"source":["# Import data and augment it from training directory\n","print(\"Augmented training data:\")\n","train_dir = \"/content/pizza_steak/train\"\n","train_data_augmented = train_datagen_augmented.flow_from_directory(train_dir,\n","                                                                   target_size=(224,224),\n","                                                                   batch_size=32,\n","                                                                   class_mode=\"binary\",\n","                                                                   shuffle=False)\n","# Create non-augmented train data batches\n","train_data = train_datagen.flow_from_directory(train_dir,\n","                                               target_size=(224,224),\n","                                               batch_size=32,\n","                                               class_mode=\"binary\",\n","                                               shuffle=False)\n","# Create non-augmented test data batchs\n","test_data_nonaugmented = test_datagen.flow_from_directory(test_dir,\n","                                                          target_size=(224,224),\n","                                                          batch_size=32,\n","                                                          class_mode=\"binary\"\n","                                                         )\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"JvM0-XFr3JGV"},"outputs":[],"source":["# Get sample augmented data batches\n","images,labels = next(train_data)\n","augmented_images,augmented_labels = next(train_data_augmented)\n","images[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Hh3w2Dkp4XlP"},"outputs":[],"source":["# Show original image and augmented image\n","import random\n","random_number = random.randint(0,31)\n","print(f\"showing image number: {random_number}\")\n","plt.imshow(images[random_number])\n","plt.title(\"Original image\")\n","plt.axis(\"off\")\n","plt.figure();\n","plt.imshow(augmented_images[random_number])\n","plt.title(\"Augmented image\")\n","plt.axis(\"off\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ijZjCWtM-_JU"},"outputs":[],"source":["# Create the model\n","model_6 = Sequential([\n","    Conv2D(10,3,activation=\"relu\"),\n","    MaxPool2D(pool_size=2),\n","    Conv2D(10,3,activation=\"relu\"),\n","    MaxPool2D(),\n","    Conv2D(10,3,activation=\"relu\"),\n","    MaxPool2D(),\n","    Flatten(),\n","    Dense(1,activation=\"sigmoid\")\n","\n","])\n","\n","# Compile the model\n","model_6.compile(loss=\"binary_crossentropy\",\n","                optimizer=Adam(),\n","                metrics=[\"accuracy\"])\n","# Fit the model\n","history_6 = model_6.fit(train_data_augmented,\n","                        epochs=5,\n","                        steps_per_epoch=len(train_data_augmented),\n","                        validation_data=test_data,\n","                        validation_steps=len(test_data))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"K4zaNzJSC9K7"},"outputs":[],"source":["# Show model_6 training curves\n","plot_loss_curves(history_6)"]},{"cell_type":"markdown","metadata":{"id":"HHQ3UE6TE41O"},"source":["## Let's shuffle our augmented training data and train another model(the same as before) on it and see what happens."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"0lGBZPKJFHGR"},"outputs":[],"source":["# Import data and augment it and shuffle from training directory\n","train_data_augmented_shuffled = train_datagen_augmented.flow_from_directory(train_dir,\n","                                                                target_size=(224,224),\n","                                                                batch_size=32,\n","                                                                class_mode=\"binary\",\n","                                                                shuffle=True # shuffle data now\n","                                                                )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"kB54WtugGuIn"},"outputs":[],"source":["train_data_augmented_shuffled"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"TBSFC09pGTlO"},"outputs":[],"source":["# Create the model\n","model_7 = Sequential([\n","    Conv2D(10,3,activation=\"relu\"),\n","    MaxPool2D(),\n","    Conv2D(10,3,activation=\"relu\"),\n","    MaxPool2D(),\n","    Conv2D(10,3,activation=\"relu\"),\n","    MaxPool2D(),\n","    Flatten(),\n","    Dense(1,activation=\"sigmoid\")\n","])\n","\n","# Compile the model\n","model_7.compile(loss=\"binary_crossentropy\",\n","                optimizer=Adam(),\n","                metrics=[\"accuracy\"])\n","# Fit the model\n","history_7 = model_7.fit(train_data_augmented_shuffled,\n","            epochs=5,\n","            steps_per_epoch=len(train_data_augmented_shuffled),\n","            validation_data=test_data,\n","            validation_steps=len(test_data)\n","            )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"rOcLF3eyHGDh"},"outputs":[],"source":["# plot loss curves of model_7\n","plot_loss_curves(history_7)"]},{"cell_type":"markdown","metadata":{"id":"tonNGiRa3FOE"},"source":["### 7.Repeat untill satisfied\n","Since we already beaten our baseline, there are a few things we could try to contune to improve our model:\n","* add more layers `Conv2D`/`MaxPool2D`\n","* Increase the number of filters in each convolutional layers( e.g from 10 to 32 or 64)\n","* Train for longer (more epochs)\n","* Find an ideal learning rate rearange tensor value\n","* Get more data (give the model more opportunities to learn)\n","* Use **Transfer Learnig** to leverage what another image model has learn and adjust it for our own use case"]},{"cell_type":"markdown","metadata":{"id":"4mmyPYAf65DZ"},"source":["## Making a prediction with our trained model on our own custom data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"pTAcHG80BLHo"},"outputs":[],"source":["# Classes we're working with\n","print(class_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"kpGdCk57BTCl"},"outputs":[],"source":["!wget https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/images/03-steak.jpeg\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"uUIQGg3HBXG7"},"outputs":[],"source":["!ls -la"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"kkQE_B63BbvC"},"outputs":[],"source":["import matplotlib.image as mpimg\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ATh6HdF6BoGc"},"outputs":[],"source":["# print(os.path.exists(\"/content/03-steak.jpeg\"))\n","# from PIL import  Image\n","# Image.open(\"/content/03-steak (1).jpeg\")\n","steak = mpimg.imread('/content/03-steak (1).jpeg')\n","plt.imshow(steak)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"GkBSQtZVFYDF"},"outputs":[],"source":["# resize the image to match what model_7 expext\n","steak_resized = tf.image.resize(steak,[224,224])\n","steak_resized = tf.cast(steak_resized,dtype=tf.int32)\n","\n","# show resized image\n","plt.imshow(steak_resized)\n","plt.axis(False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Y7jJVtePGVvh"},"outputs":[],"source":["# get prediction of resized image\n","model_7.predict(steak_resized)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"geE5yt4oLsa7"},"outputs":[],"source":["# add batch size to the image before feeding to the model\n","steak_resized = tf.expand_dims(steak_resized,axis=0)\n","model_7.predict(steak_resized)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"RtoxCZRcNbcA"},"outputs":[],"source":["# test model with Pizza image\n","pizza = mpimg.imread(\"/content/pizza.jfif\")\n","plt.imshow(pizza)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"qG2eCMWs1yDD"},"outputs":[],"source":["# Resize and add batch size to the image so as to pass model_7 requirement exam\n","resized_pizza = tf.image.resize(pizza,[224,224])\n","# show resized pizza image\n","plt.imshow(resized_pizza)\n","# add extra dimention for batch size\n","resized_batched_pizza = tf.expand_dims(resized_pizza,axis=0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"XLWmLX2w4GDy"},"outputs":[],"source":["!ls drive/MyDrive/'Colab Noteooks'"]}],"metadata":{"accelerator":"TPU","colab":{"gpuType":"V5E1","provenance":[],"toc_visible":true,"mount_file_id":"1p1GFmOkvQ1A7vFBxSdztBDI5--KouVXO","authorship_tag":"ABX9TyNupqHgX/itfDk++9Ojj+ug"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}